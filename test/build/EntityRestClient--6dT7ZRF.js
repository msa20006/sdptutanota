
globalThis.env = {
  "staticUrl": "http://localhost:9000",
  "versionNumber": "264.250130.1",
  "dist": false,
  "mode": "Test",
  "timeout": 20000,
  "domainConfigs": {
    "mail.tutanota.com": {
      "firstPartyDomain": true,
      "partneredDomainTransitionUrl": "https://app.tuta.com",
      "apiUrl": "https://mail.tutanota.com",
      "paymentUrl": "https://pay.tutanota.com/braintree.html",
      "webauthnUrl": "https://app.tuta.com/webauthn",
      "legacyWebauthnUrl": "https://mail.tutanota.com/webauthn",
      "webauthnMobileUrl": "https://app.tuta.com/webauthnmobile",
      "legacyWebauthnMobileUrl": "https://mail.tutanota.com/webauthnmobile",
      "webauthnRpId": "tutanota.com",
      "u2fAppId": "https://tutanota.com/u2f-appid.json",
      "giftCardBaseUrl": "https://app.tuta.com/giftcard",
      "referralBaseUrl": "https://app.tuta.com/signup",
      "websiteBaseUrl": "https://tutanota.com"
    },
    "test.tutanota.com": {
      "firstPartyDomain": true,
      "partneredDomainTransitionUrl": "https://app.test.tuta.com",
      "apiUrl": "https://test.tutanota.com",
      "paymentUrl": "https://pay.test.tutanota.com/braintree.html",
      "webauthnUrl": "https://app.test.tuta.com/webauthn",
      "legacyWebauthnUrl": "https://test.tutanota.com/webauthn",
      "webauthnMobileUrl": "https://app.test.tuta.com/webauthnmobile",
      "legacyWebauthnMobileUrl": "https://test.tutanota.com/webauthnmobile",
      "webauthnRpId": "tutanota.com",
      "u2fAppId": "https://test.tutanota.com/u2f-appid.json",
      "giftCardBaseUrl": "https://app.test.tuta.com/giftcard",
      "referralBaseUrl": "https://app.test.tuta.com/signup",
      "websiteBaseUrl": "https://tutanota.com"
    },
    "app.local.tutanota.com": {
      "firstPartyDomain": true,
      "partneredDomainTransitionUrl": "https://app.local.tuta.com:9000",
      "apiUrl": "https://app.local.tutanota.com:9000",
      "paymentUrl": "https://local.tutanota.com:9000/client/build/braintree.html",
      "webauthnUrl": "https://app.local.tuta.com:9000/client/build/webauthn",
      "legacyWebauthnUrl": "https://local.tutanota.com:9000/client/build/webauthn",
      "webauthnMobileUrl": "https://app.local.tuta.com:9000/client/build/webauthnmobile",
      "legacyWebauthnMobileUrl": "https://local.tutanota.com:9000/client/build/webauthnmobile",
      "webauthnRpId": "tutanota.com",
      "u2fAppId": "https://local.tutanota.com/u2f-appid.json",
      "giftCardBaseUrl": "https://app.local.tuta.com:9000/giftcard",
      "referralBaseUrl": "https://app.local.tuta.com:9000/signup",
      "websiteBaseUrl": "https://local.tutanota.com:9000"
    },
    "app.tuta.com": {
      "firstPartyDomain": true,
      "partneredDomainTransitionUrl": "https://mail.tutanota.com",
      "apiUrl": "https://app.tuta.com",
      "paymentUrl": "https://pay.tutanota.com/braintree.html",
      "webauthnUrl": "https://app.tuta.com/webauthn",
      "legacyWebauthnUrl": "https://mail.tutanota.com/webauthn",
      "webauthnMobileUrl": "https://app.tuta.com/webauthnmobile",
      "legacyWebauthnMobileUrl": "https://mail.tutanota.com/webauthnmobile",
      "webauthnRpId": "tuta.com",
      "u2fAppId": "https://app.tuta.com/u2f-appid.json",
      "giftCardBaseUrl": "https://app.tuta.com/giftcard",
      "referralBaseUrl": "https://app.tuta.com/signup",
      "websiteBaseUrl": "https://tuta.com"
    },
    "app.test.tuta.com": {
      "firstPartyDomain": true,
      "partneredDomainTransitionUrl": "https://test.tutanota.com",
      "apiUrl": "https://app.test.tuta.com",
      "paymentUrl": "https://pay.test.tutanota.com/braintree.html",
      "webauthnUrl": "https://app.test.tuta.com/webauthn",
      "legacyWebauthnUrl": "https://test.tutanota.com/webauthn",
      "webauthnMobileUrl": "https://app.test.tuta.com/webauthnmobile",
      "legacyWebauthnMobileUrl": "https://test.tutanota.com/webauthnmobile",
      "webauthnRpId": "tuta.com",
      "u2fAppId": "https://app.test.tuta.com/u2f-appid.json",
      "giftCardBaseUrl": "https://app.test.tuta.com/giftcard",
      "referralBaseUrl": "https://app.test.tuta.com/signup",
      "websiteBaseUrl": "https://test.tuta.com"
    },
    "app.local.tuta.com": {
      "firstPartyDomain": true,
      "partneredDomainTransitionUrl": "https://app.local.tutanota.com:9000",
      "apiUrl": "https://app.local.tuta.com:9000",
      "paymentUrl": "https://app.local.tuta.com:9000/braintree.html",
      "webauthnUrl": "https://app.local.tuta.com:9000/webauthn",
      "legacyWebauthnUrl": "https://local.tutanota.com:9000/webauthn",
      "webauthnMobileUrl": "https://app.local.tuta.com:9000/webauthnmobile",
      "legacyWebauthnMobileUrl": "https://local.tutanota.com:9000/webauthnmobile",
      "webauthnRpId": "tuta.com",
      "u2fAppId": "https://app.local.tuta.com/u2f-appid.json",
      "giftCardBaseUrl": "https://app.local.tuta.com:9000/giftcard",
      "referralBaseUrl": "https://app.local.tuta.com:9000/signup",
      "websiteBaseUrl": "https://local.tuta.com:9000"
    },
    "localhost": {
      "firstPartyDomain": true,
      "partneredDomainTransitionUrl": "http://localhost:9000",
      "apiUrl": "http://localhost:9000",
      "paymentUrl": "http://localhost:9000/braintree.html",
      "webauthnUrl": "http://localhost:9000/webauthn",
      "legacyWebauthnUrl": "http://localhost:9000/webauthn",
      "webauthnMobileUrl": "http://localhost:9000/webauthnmobile",
      "legacyWebauthnMobileUrl": "http://localhost:9000/webauthnmobile",
      "webauthnRpId": "localhost",
      "u2fAppId": "http://localhost:9000/u2f-appid.json",
      "giftCardBaseUrl": "http://localhost:9000/giftcard",
      "referralBaseUrl": "http://localhost:9000/signup",
      "websiteBaseUrl": "https://tuta.com"
    },
    "{hostname}": {
      "firstPartyDomain": false,
      "partneredDomainTransitionUrl": "{protocol}//{hostname}",
      "apiUrl": "{protocol}//{hostname}",
      "paymentUrl": "https://pay.tutanota.com/braintree.html",
      "webauthnUrl": "{protocol}//{hostname}/webauthn",
      "legacyWebauthnUrl": "{protocol}//{hostname}/webauthn",
      "webauthnMobileUrl": "{protocol}//{hostname}/webauthnmobile",
      "legacyWebauthnMobileUrl": "{protocol}//{hostname}/webauthnmobile",
      "webauthnRpId": "{hostname}",
      "u2fAppId": "{protocol}//{hostname}/u2f-appid.json",
      "giftCardBaseUrl": "https://app.tuta.com/giftcard",
      "referralBaseUrl": "https://app.tuta.com/signup",
      "websiteBaseUrl": "https://tuta.com"
    }
  },
  "platformId": null
};
const __NODE_GYP_better_sqlite3 = `./better-sqlite3.darwin-${typeof process !== 'undefined' ? process.arch : "unknown"}.node`
import { TypeRef, assert, assertNotNull, base64ExtToBase64, base64ToBase64Ext, base64ToBase64Url, base64UrlToBase64, binarySearch, defer, delay, difference, downcast, freezeMap, getFirstOrThrow, getTypeId, groupBy, groupByAndMapUniquely, identity, isEmpty, isSameTypeRef, lastThrow, mapNullable, ofClass, pMap, promiseFilter, randomIntFromInterval, splitInChunks } from "./dist-CJHwsXKY.js";
import { ProgrammingError } from "./ProgrammingError-D8yJGVtm.js";
import { assertMainOrNode, assertWorkerOrNode, isDesktop, isOfflineStorageAvailable, isTest } from "./Env-D5xGlXfw.js";
import { client } from "./ClientDetector-D0v6Vqu6.js";
import { CloseEventBusOption, GroupType, OperationType, SECOND_MS } from "./TutanotaConstants-3bwAESYA.js";
import { AccessBlockedError, AccessDeactivatedError, ConnectionError, InternalServerError, NotAuthenticatedError, NotAuthorizedError, NotFoundError, PayloadTooLargeError, ServiceUnavailableError, SessionExpiredError, handleRestError } from "./RestError-D17JEBMr.js";
import { LoginIncompleteError } from "./LoginIncompleteError-CpiW0a0l.js";
import { SessionKeyNotFoundError, isOfflineError, objToError } from "./ErrorUtils-o1-v67Dd.js";
import { OutOfSyncError } from "./OutOfSyncError-Ck2yBBO8.js";
import { CancelledError } from "./CancelledError-FjP5S_cR.js";
import { AssociationType, CUSTOM_MAX_ID, CUSTOM_MIN_ID, Cardinality, GENERATED_MAX_ID, GENERATED_MIN_ID, LOAD_MULTIPLE_LIMIT, POST_MULTIPLE_LIMIT, Type, ValueType, compareOldestFirst, firstBiggerThanSecond, getElementId, getListId, isSameId } from "./EntityUtils-RQxXZlcV.js";
import { CalendarEventTypeRef, CalendarEventUidIndexTypeRef, MailDetailsBlobTypeRef, MailFolderTypeRef, MailSetEntryTypeRef, MailTypeRef, PhishingMarkerWebsocketDataTypeRef } from "./TypeRefs-CR3TLWn0.js";
import { AuditLogEntryTypeRef, BucketPermissionTypeRef, EntityEventBatchTypeRef, GroupKeyTypeRef, KeyRotationTypeRef, PermissionTypeRef, PushIdentifierTypeRef, RecoverCodeTypeRef, RejectedSenderTypeRef, SecondFactorTypeRef, SessionTypeRef, UserGroupKeyDistributionTypeRef, UserGroupRootTypeRef, UserTypeRef, WebsocketCounterDataTypeRef, WebsocketEntityDataTypeRef, WebsocketLeaderStatusTypeRef, createWebsocketLeaderStatus } from "./TypeRefs-BP1jvX9p.js";
import { HttpMethod, MediaType, ModelInfo_default$1 as ModelInfo_default, _verifyType, resolveTypeReference } from "./EntityFunctions-l6CncM5C.js";
import { ModelInfo_default$2 as ModelInfo_default$1 } from "./ModelInfo-DEa-Yxv2.js";
import { SetupMultipleError } from "./SetupMultipleError-B6uY8P-x.js";
import { containsEventOfType, getEventOfType } from "./EntityUpdateUtils-B5iTKMk4.js";
import { handleUncaughtError } from "./ErrorHandler-DbW1lJbv.js";
import { EventQueue } from "./EventQueue-c-5UmjJa.js";
import { MessageDispatcher, Request } from "./MessageDispatcher-wJwFhXWv.js";
import { untagSqlObject } from "./SqlValue-CkGu32Qd.js";
import { SqlFragment, sql } from "./Sql-C9YhYNym.js";

//#region ../src/common/api/worker/ProgressMonitorDelegate.ts
var ProgressMonitorDelegate = class {
	ref;
	constructor(progressTracker, totalWork) {
		this.progressTracker = progressTracker;
		this.totalWork = totalWork;
		this.ref = progressTracker.registerMonitor(totalWork);
	}
	async workDone(amount) {
		await this.progressTracker.workDoneForMonitor(await this.ref, amount);
	}
	async totalWorkDone(totalAmount) {
		await this.progressTracker.workDoneForMonitor(await this.ref, this.totalWork - totalAmount);
	}
	async completed() {
		await this.progressTracker.workDoneForMonitor(await this.ref, this.totalWork);
	}
};

//#endregion
//#region ../src/common/api/common/threading/Transport.ts
var WebWorkerTransport = class {
	constructor(worker) {
		this.worker = worker;
	}
	postMessage(message) {
		return this.worker.postMessage(message);
	}
	setMessageHandler(handler) {
		this.worker.onmessage = (ev) => handler(downcast(ev.data));
	}
};

//#endregion
//#region ../src/common/api/common/WorkerProxy.ts
function exposeRemote(requestSender) {
	const workerProxy = new Proxy({}, { get: (_, property) => {
		return facadeProxy(requestSender, property);
	} });
	return downcast(workerProxy);
}
function exposeLocalDelayed(impls) {
	return async (message) => {
		const [facade, fn, args] = message.args;
		const init = downcast(impls)[facade];
		if (init == null) throw new ProgrammingError(`Facade is not exposed: ${facade}.${fn} (exposeLocal)`);
		const impl = await init();
		if (impl == null) throw new ProgrammingError(`Facade is not lazy: ${facade}.${fn} (exposeLocalDelayed)`);
		return downcast(impl)[fn](...args);
	};
}
/**
* Generates proxy which will generate methods which will simulate methods of the facade.
*/
function facadeProxy(requestSender, facadeName) {
	return new Proxy({}, { get: (_, property) => {
		if (property === "then") return undefined;
else return (...args) => {
			const request = new Request("facade", [
				facadeName,
				property,
				args
			]);
			return requestSender(request);
		};
	} });
}

//#endregion
//#region ../src/common/api/main/WorkerClient.ts
assertMainOrNode();
let WsConnectionState = function(WsConnectionState$1) {
	WsConnectionState$1[WsConnectionState$1["connecting"] = 0] = "connecting";
	WsConnectionState$1[WsConnectionState$1["connected"] = 1] = "connected";
	WsConnectionState$1[WsConnectionState$1["terminated"] = 2] = "terminated";
	return WsConnectionState$1;
}({});
var WorkerClient = class {
	_deferredInitialized = defer();
	_isInitialized = false;
	_dispatcher;
	constructor() {
		this.initialized.then(() => {
			this._isInitialized = true;
		});
	}
	get initialized() {
		return this._deferredInitialized.promise;
	}
	async init(locator) {
		if (env.mode !== "Test") {
			const { prefixWithoutFile } = window.tutao.appState;
			const workerUrl = prefixWithoutFile + "/worker-bootstrap.js";
			const worker = new Worker(workerUrl, { type: "module" });
			this._dispatcher = new MessageDispatcher(new WebWorkerTransport(worker), this.queueCommands(locator), "main-worker");
			await this._dispatcher.postRequest(new Request("setup", [
				window.env,
				this.getInitialEntropy(),
				client.browserData()
			]));
			worker.onerror = (e) => {
				throw new Error(`could not setup worker: ${e.name} ${e.stack} ${e.message} ${e}`);
			};
		} else {
			const WorkerImpl = globalThis.testWorker;
			const workerImpl = new WorkerImpl(this, true);
			await workerImpl.init(client.browserData());
			workerImpl._queue._transport = { postMessage: (msg) => this._dispatcher.handleMessage(msg) };
			this._dispatcher = new MessageDispatcher({ postMessage: function(msg) {
				workerImpl._queue.handleMessage(msg);
			} }, this.queueCommands(locator), "main-worker");
		}
		this._deferredInitialized.resolve();
	}
	queueCommands(locator) {
		return {
			execNative: (message) => locator.native.invokeNative(downcast(message.args[0]), downcast(message.args[1])),
			error: (message) => {
				handleUncaughtError(objToError(message.args[0]));
				return Promise.resolve();
			},
			facade: exposeLocalDelayed({
				async loginListener() {
					return locator.loginListener;
				},
				async wsConnectivityListener() {
					return locator.connectivityModel;
				},
				async progressTracker() {
					return locator.progressTracker;
				},
				async eventController() {
					return locator.eventController;
				},
				async operationProgressTracker() {
					return locator.operationProgressTracker;
				},
				async infoMessageHandler() {
					return locator.infoMessageHandler;
				}
			})
		};
	}
	getWorkerInterface() {
		return exposeRemote(async (request) => this._postRequest(request));
	}
	restRequest(...args) {
		return this._postRequest(new Request("restRequest", args));
	}
	/** @private visible for tests */
	async _postRequest(msg) {
		await this.initialized;
		return this._dispatcher.postRequest(msg);
	}
	reset() {
		return this._postRequest(new Request("reset", []));
	}
	/**
	* Add data from either secure random source or Math.random as entropy.
	*/
	getInitialEntropy() {
		const valueList = new Uint32Array(16);
		crypto.getRandomValues(valueList);
		const entropy = [];
		for (let i = 0; i < valueList.length; i++) entropy.push({
			source: "random",
			entropy: 32,
			data: valueList[i]
		});
		return entropy;
	}
};
function bootstrapWorker(locator) {
	const worker = new WorkerClient();
	const start = Date.now();
	worker.init(locator).then(() => console.log("worker init time (ms):", Date.now() - start));
	return worker;
}

//#endregion
//#region ../src/common/api/worker/EventBusClient.ts
assertWorkerOrNode();
let EventBusState = function(EventBusState$1) {
	EventBusState$1["Automatic"] = "automatic";
	EventBusState$1["Suspended"] = "suspended";
	EventBusState$1["Terminated"] = "terminated";
	return EventBusState$1;
}({});
const ENTITY_EVENT_BATCH_EXPIRE_MS = 38016e5;
const RETRY_AFTER_SERVICE_UNAVAILABLE_ERROR_MS = 3e4;
const NORMAL_SHUTDOWN_CLOSE_CODE = 1;
/**
* Reconnection interval bounds. When we reconnect we pick a random number of seconds in a range to prevent that all the clients connect at the same time which
* would put unnecessary load on the server.
* The range depends on the number of attempts and the server response.
* */
const RECONNECT_INTERVAL = Object.freeze({
	SMALL: [5, 10],
	MEDIUM: [20, 40],
	LARGE: [60, 120]
});
const MAX_EVENT_IDS_QUEUE_LENGTH = 1e3;
/** Known types of messages that can be received over websocket. */
var MessageType = function(MessageType$1) {
	MessageType$1["EntityUpdate"] = "entityUpdate";
	MessageType$1["UnreadCounterUpdate"] = "unreadCounterUpdate";
	MessageType$1["PhishingMarkers"] = "phishingMarkers";
	MessageType$1["LeaderStatus"] = "leaderStatus";
	return MessageType$1;
}(MessageType || {});
let ConnectMode = function(ConnectMode$1) {
	ConnectMode$1[ConnectMode$1["Initial"] = 0] = "Initial";
	ConnectMode$1[ConnectMode$1["Reconnect"] = 1] = "Reconnect";
	return ConnectMode$1;
}({});
var EventBusClient = class {
	state;
	socket;
	immediateReconnect = false;
	/**
	* Map from group id to last event ids (max. _MAX_EVENT_IDS_QUEUE_LENGTH). We keep them to avoid processing the same event twice if
	* it comes out of order from the server) and for requesting missed entity events on reconnect.
	*
	* We do not have to update these event ids if the groups of the user change because we always take the current users groups from the
	* LoginFacade.
	*/
	lastEntityEventIds;
	/**
	* Last batch which was actually added to the queue. We need it to find out when the group is processed
	*/
	lastAddedBatchForGroup;
	lastAntiphishingMarkersId = null;
	/** Queue to process all events. */
	eventQueue;
	/** Queue that handles incoming websocket messages only. Caches them until we process downloaded ones and then adds them to eventQueue. */
	entityUpdateMessageQueue;
	reconnectTimer;
	connectTimer;
	/**
	* Represents a currently retried executing due to a ServiceUnavailableError
	*/
	serviceUnavailableRetry = null;
	failedConnectionAttempts = 0;
	constructor(listener, cache, userFacade, entity, instanceMapper, socketFactory, sleepDetector, progressTracker) {
		this.listener = listener;
		this.cache = cache;
		this.userFacade = userFacade;
		this.entity = entity;
		this.instanceMapper = instanceMapper;
		this.socketFactory = socketFactory;
		this.sleepDetector = sleepDetector;
		this.progressTracker = progressTracker;
		this.state = EventBusState.Terminated;
		this.lastEntityEventIds = new Map();
		this.lastAddedBatchForGroup = new Map();
		this.socket = null;
		this.reconnectTimer = null;
		this.connectTimer = null;
		this.eventQueue = new EventQueue("ws_opt", true, (modification) => this.eventQueueCallback(modification));
		this.entityUpdateMessageQueue = new EventQueue("ws_msg", false, (batch) => this.entityUpdateMessageQueueCallback(batch));
		this.reset();
	}
	reset() {
		this.immediateReconnect = false;
		this.lastEntityEventIds.clear();
		this.lastAddedBatchForGroup.clear();
		this.eventQueue.pause();
		this.eventQueue.clear();
		this.serviceUnavailableRetry = null;
	}
	/**
	* Opens a WebSocket connection to receive server events.
	* @param connectMode
	*/
	connect(connectMode) {
		console.log("ws connect reconnect:", connectMode === ConnectMode.Reconnect, "state:", this.state);
		this.serviceUnavailableRetry = null;
		this.listener.onWebsocketStateChanged(WsConnectionState.connecting);
		this.state = EventBusState.Automatic;
		this.connectTimer = null;
		const authHeaders = this.userFacade.createAuthHeaders();
		const authQuery = "modelVersions=" + ModelInfo_default$1.version + "." + ModelInfo_default.version + "&clientVersion=" + env.versionNumber + "&userId=" + this.userFacade.getLoggedInUser()._id + "&accessToken=" + authHeaders.accessToken + (this.lastAntiphishingMarkersId ? "&lastPhishingMarkersId=" + this.lastAntiphishingMarkersId : "");
		const path = "/event?" + authQuery;
		this.unsubscribeFromOldWebsocket();
		this.socket = this.socketFactory(path);
		this.socket.onopen = () => this.onOpen(connectMode);
		this.socket.onclose = (event) => this.onClose(event);
		this.socket.onerror = (error) => this.onError(error);
		this.socket.onmessage = (message) => this.onMessage(message);
		this.sleepDetector.start(() => {
			console.log("ws sleep detected, reconnecting...");
			this.tryReconnect(true, true);
		});
	}
	/**
	* Sends a close event to the server and finally closes the connection.
	* The state of this event bus client is reset and the client is terminated (does not automatically reconnect) except reconnect == true
	*/
	async close(closeOption) {
		console.log("ws close closeOption: ", closeOption, "state:", this.state);
		switch (closeOption) {
			case CloseEventBusOption.Terminate:
				this.terminate();
				break;
			case CloseEventBusOption.Pause:
				this.state = EventBusState.Suspended;
				this.listener.onWebsocketStateChanged(WsConnectionState.connecting);
				break;
			case CloseEventBusOption.Reconnect:
				this.listener.onWebsocketStateChanged(WsConnectionState.connecting);
				break;
		}
		this.socket?.close();
	}
	async tryReconnect(closeIfOpen, enableAutomaticState, delay$1 = null) {
		console.log("ws tryReconnect closeIfOpen:", closeIfOpen, "enableAutomaticState:", enableAutomaticState, "delay:", delay$1);
		if (this.reconnectTimer) {
			clearTimeout(this.reconnectTimer);
			this.reconnectTimer = null;
		}
		if (!delay$1) this.reconnect(closeIfOpen, enableAutomaticState);
else this.reconnectTimer = setTimeout(() => this.reconnect(closeIfOpen, enableAutomaticState), delay$1);
	}
	onOpen(connectMode) {
		this.failedConnectionAttempts = 0;
		console.log("ws open state:", this.state);
		const p = this.initEntityEvents(connectMode);
		this.listener.onWebsocketStateChanged(WsConnectionState.connected);
		return p;
	}
	onError(error) {
		console.log("ws error:", error, JSON.stringify(error), "state:", this.state);
	}
	async onMessage(message) {
		const [type, value] = message.data.split(";");
		switch (type) {
			case MessageType.EntityUpdate: {
				const { eventBatchId, eventBatchOwner, eventBatch } = await this.instanceMapper.decryptAndMapToInstance(await resolveTypeReference(WebsocketEntityDataTypeRef), JSON.parse(value), null);
				const filteredEntityUpdates = await this.removeUnknownTypes(eventBatch);
				this.entityUpdateMessageQueue.add(eventBatchId, eventBatchOwner, filteredEntityUpdates);
				break;
			}
			case MessageType.UnreadCounterUpdate: {
				const counterData = await this.instanceMapper.decryptAndMapToInstance(await resolveTypeReference(WebsocketCounterDataTypeRef), JSON.parse(value), null);
				this.listener.onCounterChanged(counterData);
				break;
			}
			case MessageType.PhishingMarkers: {
				const data = await this.instanceMapper.decryptAndMapToInstance(await resolveTypeReference(PhishingMarkerWebsocketDataTypeRef), JSON.parse(value), null);
				this.lastAntiphishingMarkersId = data.lastId;
				this.listener.onPhishingMarkersReceived(data.markers);
				break;
			}
			case MessageType.LeaderStatus: {
				const data = await this.instanceMapper.decryptAndMapToInstance(await resolveTypeReference(WebsocketLeaderStatusTypeRef), JSON.parse(value), null);
				await this.userFacade.setLeaderStatus(data);
				await this.listener.onLeaderStatusChanged(data);
				break;
			}
			default:
				console.log("ws message with unknown type", type);
				break;
		}
	}
	/**
	* Filters out specific types from @param entityUpdates that the client does not actually know about
	* (that are not in tutanotaTypes), and which should therefore not be processed.
	*/
	async removeUnknownTypes(eventBatch) {
		return promiseFilter(eventBatch, async (entityUpdate) => {
			const typeRef = new TypeRef(entityUpdate.application, entityUpdate.type);
			try {
				await resolveTypeReference(typeRef);
				return true;
			} catch (_error) {
				console.warn("ignoring entityEventUpdate for unknown type with typeId", getTypeId(typeRef));
				return false;
			}
		});
	}
	onClose(event) {
		this.failedConnectionAttempts++;
		console.log("ws close event:", event, "state:", this.state);
		this.userFacade.setLeaderStatus(createWebsocketLeaderStatus({ leaderStatus: false }));
		this.sleepDetector.stop();
		const serverCode = event.code - 4e3;
		if ([
			NotAuthorizedError.CODE,
			AccessDeactivatedError.CODE,
			AccessBlockedError.CODE
		].includes(serverCode)) {
			this.terminate();
			this.listener.onError(handleRestError(serverCode, "web socket error", null, null));
		} else if (serverCode === SessionExpiredError.CODE) {
			this.state = EventBusState.Suspended;
			this.listener.onWebsocketStateChanged(WsConnectionState.connecting);
		} else if (this.state === EventBusState.Automatic && this.userFacade.isFullyLoggedIn()) {
			this.listener.onWebsocketStateChanged(WsConnectionState.connecting);
			if (this.immediateReconnect) {
				this.immediateReconnect = false;
				this.tryReconnect(false, false);
			} else {
				let reconnectionInterval;
				if (serverCode === NORMAL_SHUTDOWN_CLOSE_CODE) reconnectionInterval = RECONNECT_INTERVAL.LARGE;
else if (this.failedConnectionAttempts === 1) reconnectionInterval = RECONNECT_INTERVAL.SMALL;
else if (this.failedConnectionAttempts === 2) reconnectionInterval = RECONNECT_INTERVAL.MEDIUM;
else reconnectionInterval = RECONNECT_INTERVAL.LARGE;
				this.tryReconnect(false, false, SECOND_MS * randomIntFromInterval(reconnectionInterval[0], reconnectionInterval[1]));
			}
		}
	}
	async initEntityEvents(connectMode) {
		this.entityUpdateMessageQueue.pause();
		this.eventQueue.pause();
		const existingConnection = connectMode == ConnectMode.Reconnect && this.lastEntityEventIds.size > 0;
		const p = existingConnection ? this.loadMissedEntityEvents(this.eventQueue) : this.initOnNewConnection();
		return p.then(() => {
			this.entityUpdateMessageQueue.resume();
			this.eventQueue.resume();
		}).catch(ofClass(ConnectionError, (e) => {
			console.log("ws not connected in connect(), close websocket", e);
			this.close(CloseEventBusOption.Reconnect);
		})).catch(ofClass(CancelledError, () => {
			console.log("ws cancelled retry process entity events after reconnect");
		})).catch(ofClass(ServiceUnavailableError, async (e) => {
			if (!existingConnection) this.lastEntityEventIds.clear();
			console.log("ws retry init entity events in ", RETRY_AFTER_SERVICE_UNAVAILABLE_ERROR_MS, e);
			let promise = delay(RETRY_AFTER_SERVICE_UNAVAILABLE_ERROR_MS).then(() => {
				if (this.serviceUnavailableRetry === promise) {
					console.log("ws retry initializing entity events");
					return this.initEntityEvents(connectMode);
				} else console.log("ws cancel initializing entity events");
			});
			this.serviceUnavailableRetry = promise;
			return promise;
		})).catch(ofClass(OutOfSyncError, async (e) => {
			await this.cache.purgeStorage();
			throw e;
		})).catch((e) => {
			this.entityUpdateMessageQueue.resume();
			this.eventQueue.resume();
			this.listener.onError(e);
		});
	}
	async initOnNewConnection() {
		const { lastIds, someIdsWereCached } = await this.retrieveLastEntityEventIds();
		this.lastEntityEventIds = lastIds;
		if (someIdsWereCached) await this.loadMissedEntityEvents(this.eventQueue);
else await this.cache.recordSyncTime();
	}
	/**
	* Gets the latest event batch ids for each of the users groups or min id if there is no event batch yet.
	* This is needed to know from where to start loading missed events when we connect.
	*/
	async retrieveLastEntityEventIds() {
		const lastIds = new Map();
		let someIdsWereCached = false;
		for (const groupId of this.eventGroups()) {
			const cachedBatchId = await this.cache.getLastEntityEventBatchForGroup(groupId);
			if (cachedBatchId != null) {
				lastIds.set(groupId, [cachedBatchId]);
				someIdsWereCached = true;
			} else {
				const batches = await this.entity.loadRange(EntityEventBatchTypeRef, groupId, GENERATED_MAX_ID, 1, true);
				const batchId = batches.length === 1 ? getElementId(batches[0]) : GENERATED_MIN_ID;
				lastIds.set(groupId, [batchId]);
				await this.cache.setLastEntityEventBatchForGroup(groupId, batchId);
			}
		}
		return {
			lastIds,
			someIdsWereCached
		};
	}
	/** Load event batches since the last time we were connected to bring cache and other things up-to-date.
	* @param eventQueue is passed in for testing
	* @VisibleForTesting
	* */
	async loadMissedEntityEvents(eventQueue) {
		if (!this.userFacade.isFullyLoggedIn()) return;
		await this.checkOutOfSync();
		let eventBatches = [];
		for (let groupId of this.eventGroups()) {
			const eventBatchForGroup = await this.loadEntityEventsForGroup(groupId);
			eventBatches = eventBatches.concat(eventBatchForGroup);
		}
		const timeSortedEventBatches = eventBatches.sort((a, b) => compareOldestFirst(getElementId(a), getElementId(b)));
		let totalExpectedBatches = 0;
		for (const batch of timeSortedEventBatches) {
			const filteredEntityUpdates = await this.removeUnknownTypes(batch.events);
			const batchWasAddedToQueue = this.addBatch(getElementId(batch), getListId(batch), filteredEntityUpdates, eventQueue);
			if (batchWasAddedToQueue) totalExpectedBatches++;
		}
		const progressMonitor = new ProgressMonitorDelegate(this.progressTracker, totalExpectedBatches + 1);
		console.log("ws", `progress monitor expects ${totalExpectedBatches} events`);
		await progressMonitor.workDone(1);
		eventQueue.setProgressMonitor(progressMonitor);
		await this.cache.recordSyncTime();
	}
	async loadEntityEventsForGroup(groupId) {
		try {
			return await this.entity.loadAll(EntityEventBatchTypeRef, groupId, this.getLastEventBatchIdOrMinIdForGroup(groupId));
		} catch (e) {
			if (e instanceof NotAuthorizedError) {
				console.log("ws could not download entity updates, lost permission");
				return [];
			} else throw e;
		}
	}
	async checkOutOfSync() {
		if (await this.cache.isOutOfSync()) throw new OutOfSyncError("some missed EntityEventBatches cannot be loaded any more");
	}
	async eventQueueCallback(modification) {
		try {
			await this.processEventBatch(modification);
		} catch (e) {
			console.log("ws error while processing event batches", e);
			this.listener.onError(e);
			throw e;
		}
	}
	async entityUpdateMessageQueueCallback(batch) {
		this.addBatch(batch.batchId, batch.groupId, batch.events, this.eventQueue);
		this.eventQueue.resume();
	}
	unsubscribeFromOldWebsocket() {
		if (this.socket) this.socket.onopen = this.socket.onclose = this.socket.onerror = this.socket.onmessage = identity;
	}
	async terminate() {
		this.state = EventBusState.Terminated;
		this.reset();
		this.listener.onWebsocketStateChanged(WsConnectionState.terminated);
	}
	/**
	* Tries to reconnect the websocket if it is not connected.
	*/
	reconnect(closeIfOpen, enableAutomaticState) {
		console.log("ws reconnect socket.readyState: (CONNECTING=0, OPEN=1, CLOSING=2, CLOSED=3): " + (this.socket ? this.socket.readyState : "null"), "state:", this.state, "closeIfOpen:", closeIfOpen, "enableAutomaticState:", enableAutomaticState);
		if (this.state !== EventBusState.Terminated && enableAutomaticState) this.state = EventBusState.Automatic;
		if (closeIfOpen && this.socket && this.socket.readyState === WebSocket.OPEN) {
			this.immediateReconnect = true;
			this.socket.close();
		} else if ((this.socket == null || this.socket.readyState === WebSocket.CLOSED || this.socket.readyState === WebSocket.CLOSING) && this.state !== EventBusState.Terminated && this.userFacade.isFullyLoggedIn()) {
			if (this.connectTimer) clearTimeout(this.connectTimer);
			this.connectTimer = setTimeout(() => this.connect(ConnectMode.Reconnect), 100);
		}
	}
	addBatch(batchId, groupId, events, eventQueue) {
		const lastForGroup = this.lastEntityEventIds.get(groupId) || [];
		const index = binarySearch(lastForGroup, batchId, compareOldestFirst);
		let wasAdded;
		if (index < 0) {
			lastForGroup.splice(-index, 0, batchId);
			wasAdded = eventQueue.add(batchId, groupId, events);
		} else wasAdded = false;
		if (lastForGroup.length > MAX_EVENT_IDS_QUEUE_LENGTH) lastForGroup.shift();
		this.lastEntityEventIds.set(batchId, lastForGroup);
		if (wasAdded) this.lastAddedBatchForGroup.set(groupId, batchId);
		return wasAdded;
	}
	async processEventBatch(batch) {
		try {
			if (this.isTerminated()) return;
			const filteredEvents = await this.cache.entityEventsReceived(batch);
			if (!this.isTerminated()) await this.listener.onEntityEventsReceived(filteredEvents, batch.batchId, batch.groupId);
		} catch (e) {
			if (e instanceof ServiceUnavailableError) {
				console.log("ws retry processing event in 30s", e);
				const retryPromise = delay(RETRY_AFTER_SERVICE_UNAVAILABLE_ERROR_MS).then(() => {
					if (this.serviceUnavailableRetry === retryPromise) return this.processEventBatch(batch);
else throw new CancelledError("stop retry processing after service unavailable due to reconnect");
				});
				this.serviceUnavailableRetry = retryPromise;
				return retryPromise;
			} else {
				console.log("EVENT", "error", e);
				throw e;
			}
		}
	}
	getLastEventBatchIdOrMinIdForGroup(groupId) {
		const lastIds = this.lastEntityEventIds.get(groupId);
		return lastIds && lastIds.length > 0 ? lastThrow(lastIds) : GENERATED_MIN_ID;
	}
	isTerminated() {
		return this.state === EventBusState.Terminated;
	}
	eventGroups() {
		return this.userFacade.getLoggedInUser().memberships.filter((membership) => membership.groupType !== GroupType.MailingList).map((membership) => membership.group).concat(this.userFacade.getLoggedInUser().userGroup.group);
	}
};

//#endregion
//#region ../libs/cborg.js
const typeofs = [
	"string",
	"number",
	"bigint",
	"symbol"
];
const objectTypeNames = [
	"Function",
	"Generator",
	"AsyncGenerator",
	"GeneratorFunction",
	"AsyncGeneratorFunction",
	"AsyncFunction",
	"Observable",
	"Array",
	"Buffer",
	"Object",
	"RegExp",
	"Date",
	"Error",
	"Map",
	"Set",
	"WeakMap",
	"WeakSet",
	"ArrayBuffer",
	"SharedArrayBuffer",
	"DataView",
	"Promise",
	"URL",
	"HTMLElement",
	"Int8Array",
	"Uint8Array",
	"Uint8ClampedArray",
	"Int16Array",
	"Uint16Array",
	"Int32Array",
	"Uint32Array",
	"Float32Array",
	"Float64Array",
	"BigInt64Array",
	"BigUint64Array"
];
/**
* @param {any} value
* @returns {string}
*/
function is(value) {
	if (value === null) return "null";
	if (value === undefined) return "undefined";
	if (value === true || value === false) return "boolean";
	const typeOf = typeof value;
	if (typeofs.includes(typeOf)) return typeOf;
	if (typeOf === "function") return "Function";
	if (Array.isArray(value)) return "Array";
	if (isBuffer$1(value)) return "Buffer";
	const objectType = getObjectType(value);
	if (objectType) return objectType;
	return "Object";
}
/**
* @param {any} value
* @returns {boolean}
*/
function isBuffer$1(value) {
	return value && value.constructor && value.constructor.isBuffer && value.constructor.isBuffer.call(null, value);
}
/**
* @param {any} value
* @returns {string|undefined}
*/
function getObjectType(value) {
	const objectTypeName = Object.prototype.toString.call(value).slice(8, -1);
	if (objectTypeNames.includes(objectTypeName)) return objectTypeName;
	return undefined;
}
var Type$1 = class {
	/**
	* @param {number} major
	* @param {string} name
	* @param {boolean} terminal
	*/
	constructor(major, name, terminal) {
		this.major = major;
		this.majorEncoded = major << 5;
		this.name = name;
		this.terminal = terminal;
	}
	toString() {
		return `Type[${this.major}].${this.name}`;
	}
	/**
	* @param {Type} typ
	* @returns {number}
	*/
	compare(typ) {
		return this.major < typ.major ? -1 : this.major > typ.major ? 1 : 0;
	}
};
Type$1.uint = new Type$1(0, "uint", true);
Type$1.negint = new Type$1(1, "negint", true);
Type$1.bytes = new Type$1(2, "bytes", true);
Type$1.string = new Type$1(3, "string", true);
Type$1.array = new Type$1(4, "array", false);
Type$1.map = new Type$1(5, "map", false);
Type$1.tag = new Type$1(6, "tag", false);
Type$1.float = new Type$1(7, "float", true);
Type$1.false = new Type$1(7, "false", true);
Type$1.true = new Type$1(7, "true", true);
Type$1.null = new Type$1(7, "null", true);
Type$1.undefined = new Type$1(7, "undefined", true);
Type$1.break = new Type$1(7, "break", true);
var Token = class {
	/**
	* @param {Type} type
	* @param {any} [value]
	* @param {number} [encodedLength]
	*/
	constructor(type, value, encodedLength) {
		this.type = type;
		this.value = value;
		this.encodedLength = encodedLength;
		/** @type {Uint8Array|undefined} */
		this.encodedBytes = undefined;
		/** @type {Uint8Array|undefined} */
		this.byteValue = undefined;
	}
	toString() {
		return `Token[${this.type}].${this.value}`;
	}
};
const useBuffer = globalThis.process && !globalThis.process.browser && globalThis.Buffer && typeof globalThis.Buffer.isBuffer === "function";
const textDecoder = new TextDecoder();
const textEncoder = new TextEncoder();
/**
* @param {Uint8Array} buf
* @returns {boolean}
*/
function isBuffer(buf$1) {
	return useBuffer && globalThis.Buffer.isBuffer(buf$1);
}
/**
* @param {Uint8Array|number[]} buf
* @returns {Uint8Array}
*/
function asU8A(buf$1) {
	if (!(buf$1 instanceof Uint8Array)) return Uint8Array.from(buf$1);
	return isBuffer(buf$1) ? new Uint8Array(buf$1.buffer, buf$1.byteOffset, buf$1.byteLength) : buf$1;
}
const toString = useBuffer ? (bytes, start, end) => {
	return end - start > 64 ? globalThis.Buffer.from(bytes.subarray(start, end)).toString("utf8") : utf8Slice(bytes, start, end);
} : (bytes, start, end) => {
	return end - start > 64 ? textDecoder.decode(bytes.subarray(start, end)) : utf8Slice(bytes, start, end);
};
const fromString = useBuffer ? (string) => {
	return string.length > 64 ? globalThis.Buffer.from(string) : utf8ToBytes(string);
} : (string) => {
	return string.length > 64 ? textEncoder.encode(string) : utf8ToBytes(string);
};
/**
* Buffer variant not fast enough for what we need
* @param {number[]} arr
* @returns {Uint8Array}
*/
const fromArray = (arr) => {
	return Uint8Array.from(arr);
};
const slice = useBuffer ? (bytes, start, end) => {
	if (isBuffer(bytes)) return new Uint8Array(bytes.subarray(start, end));
	return bytes.slice(start, end);
} : (bytes, start, end) => {
	return bytes.slice(start, end);
};
const concat = useBuffer ? (chunks, length) => {
	chunks = chunks.map((c) => c instanceof Uint8Array ? c : globalThis.Buffer.from(c));
	return asU8A(globalThis.Buffer.concat(chunks, length));
} : (chunks, length) => {
	const out = new Uint8Array(length);
	let off = 0;
	for (let b of chunks) {
		if (off + b.length > out.length) b = b.subarray(0, out.length - off);
		out.set(b, off);
		off += b.length;
	}
	return out;
};
const alloc = useBuffer ? (size) => {
	return globalThis.Buffer.allocUnsafe(size);
} : (size) => {
	return new Uint8Array(size);
};
/**
* @param {Uint8Array} b1
* @param {Uint8Array} b2
* @returns {number}
*/
function compare(b1, b2) {
	if (isBuffer(b1) && isBuffer(b2)) return b1.compare(b2);
	for (let i = 0; i < b1.length; i++) {
		if (b1[i] === b2[i]) continue;
		return b1[i] < b2[i] ? -1 : 1;
	}
	return 0;
}
/**
* @param {string} str
* @returns {number[]}
*/
function utf8ToBytes(str) {
	const out = [];
	let p = 0;
	for (let i = 0; i < str.length; i++) {
		let c = str.charCodeAt(i);
		if (c < 128) out[p++] = c;
else if (c < 2048) {
			out[p++] = c >> 6 | 192;
			out[p++] = c & 63 | 128;
		} else if ((c & 64512) === 55296 && i + 1 < str.length && (str.charCodeAt(i + 1) & 64512) === 56320) {
			c = 65536 + ((c & 1023) << 10) + (str.charCodeAt(++i) & 1023);
			out[p++] = c >> 18 | 240;
			out[p++] = c >> 12 & 63 | 128;
			out[p++] = c >> 6 & 63 | 128;
			out[p++] = c & 63 | 128;
		} else {
			out[p++] = c >> 12 | 224;
			out[p++] = c >> 6 & 63 | 128;
			out[p++] = c & 63 | 128;
		}
	}
	return out;
}
/**
* @param {Uint8Array} buf
* @param {number} offset
* @param {number} end
* @returns {string}
*/
function utf8Slice(buf$1, offset, end) {
	const res = [];
	while (offset < end) {
		const firstByte = buf$1[offset];
		let codePoint = null;
		let bytesPerSequence = firstByte > 239 ? 4 : firstByte > 223 ? 3 : firstByte > 191 ? 2 : 1;
		if (offset + bytesPerSequence <= end) {
			let secondByte, thirdByte, fourthByte, tempCodePoint;
			switch (bytesPerSequence) {
				case 1:
					if (firstByte < 128) codePoint = firstByte;
					break;
				case 2:
					secondByte = buf$1[offset + 1];
					if ((secondByte & 192) === 128) {
						tempCodePoint = (firstByte & 31) << 6 | secondByte & 63;
						if (tempCodePoint > 127) codePoint = tempCodePoint;
					}
					break;
				case 3:
					secondByte = buf$1[offset + 1];
					thirdByte = buf$1[offset + 2];
					if ((secondByte & 192) === 128 && (thirdByte & 192) === 128) {
						tempCodePoint = (firstByte & 15) << 12 | (secondByte & 63) << 6 | thirdByte & 63;
						if (tempCodePoint > 2047 && (tempCodePoint < 55296 || tempCodePoint > 57343)) codePoint = tempCodePoint;
					}
					break;
				case 4:
					secondByte = buf$1[offset + 1];
					thirdByte = buf$1[offset + 2];
					fourthByte = buf$1[offset + 3];
					if ((secondByte & 192) === 128 && (thirdByte & 192) === 128 && (fourthByte & 192) === 128) {
						tempCodePoint = (firstByte & 15) << 18 | (secondByte & 63) << 12 | (thirdByte & 63) << 6 | fourthByte & 63;
						if (tempCodePoint > 65535 && tempCodePoint < 1114112) codePoint = tempCodePoint;
					}
			}
		}
		if (codePoint === null) {
			codePoint = 65533;
			bytesPerSequence = 1;
		} else if (codePoint > 65535) {
			codePoint -= 65536;
			res.push(codePoint >>> 10 & 1023 | 55296);
			codePoint = 56320 | codePoint & 1023;
		}
		res.push(codePoint);
		offset += bytesPerSequence;
	}
	return decodeCodePointsArray(res);
}
const MAX_ARGUMENTS_LENGTH = 4096;
/**
* @param {number[]} codePoints
* @returns {string}
*/
function decodeCodePointsArray(codePoints) {
	const len = codePoints.length;
	if (len <= MAX_ARGUMENTS_LENGTH) return String.fromCharCode.apply(String, codePoints);
	let res = "";
	let i = 0;
	while (i < len) res += String.fromCharCode.apply(String, codePoints.slice(i, i += MAX_ARGUMENTS_LENGTH));
	return res;
}
/**
* Bl is a list of byte chunks, similar to https://github.com/rvagg/bl but for
* writing rather than reading.
* A Bl object accepts set() operations for individual bytes and copyTo() for
* inserting byte arrays. These write operations don't automatically increment
* the internal cursor so its "length" won't be changed. Instead, increment()
* must be called to extend its length to cover the inserted data.
* The toBytes() call will convert all internal memory to a single Uint8Array of
* the correct length, truncating any data that is stored but hasn't been
* included by an increment().
* get() can retrieve a single byte.
* All operations (except toBytes()) take an "offset" argument that will perform
* the write at the offset _from the current cursor_. For most operations this
* will be `0` to write at the current cursor position but it can be ahead of
* the current cursor. Negative offsets probably work but are untested.
*/
const defaultChunkSize = 256;
var Bl = class {
	/**
	* @param {number} [chunkSize]
	*/
	constructor(chunkSize = defaultChunkSize) {
		this.chunkSize = chunkSize;
		/** @type {number} */
		this.cursor = 0;
		/** @type {number} */
		this.maxCursor = -1;
		/** @type {(Uint8Array|number[])[]} */
		this.chunks = [];
		/** @type {Uint8Array|number[]|null} */
		this._initReuseChunk = null;
	}
	reset() {
		this.cursor = 0;
		this.maxCursor = -1;
		if (this.chunks.length) this.chunks = [];
		if (this._initReuseChunk !== null) {
			this.chunks.push(this._initReuseChunk);
			this.maxCursor = this._initReuseChunk.length - 1;
		}
	}
	/**
	* @param {Uint8Array|number[]} bytes
	*/
	push(bytes) {
		let topChunk = this.chunks[this.chunks.length - 1];
		const newMax = this.cursor + bytes.length;
		if (newMax <= this.maxCursor + 1) {
			const chunkPos = topChunk.length - (this.maxCursor - this.cursor) - 1;
			topChunk.set(bytes, chunkPos);
		} else {
			if (topChunk) {
				const chunkPos = topChunk.length - (this.maxCursor - this.cursor) - 1;
				if (chunkPos < topChunk.length) {
					this.chunks[this.chunks.length - 1] = topChunk.subarray(0, chunkPos);
					this.maxCursor = this.cursor - 1;
				}
			}
			if (bytes.length < 64 && bytes.length < this.chunkSize) {
				topChunk = alloc(this.chunkSize);
				this.chunks.push(topChunk);
				this.maxCursor += topChunk.length;
				if (this._initReuseChunk === null) this._initReuseChunk = topChunk;
				topChunk.set(bytes, 0);
			} else {
				this.chunks.push(bytes);
				this.maxCursor += bytes.length;
			}
		}
		this.cursor += bytes.length;
	}
	/**
	* @param {boolean} [reset]
	* @returns {Uint8Array}
	*/
	toBytes(reset = false) {
		let byts;
		if (this.chunks.length === 1) {
			const chunk = this.chunks[0];
			if (reset && this.cursor > chunk.length / 2) {
				byts = this.cursor === chunk.length ? chunk : chunk.subarray(0, this.cursor);
				this._initReuseChunk = null;
				this.chunks = [];
			} else byts = slice(chunk, 0, this.cursor);
		} else byts = concat(this.chunks, this.cursor);
		if (reset) this.reset();
		return byts;
	}
};
const decodeErrPrefix = "CBOR decode error:";
const encodeErrPrefix = "CBOR encode error:";
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} need
*/
function assertEnoughData(data, pos, need) {
	if (data.length - pos < need) throw new Error(`${decodeErrPrefix} not enough data for type`);
}
const uintBoundaries = [
	24,
	256,
	65536,
	4294967296,
	BigInt("18446744073709551616")
];
/**
* @typedef {import('./bl.js').Bl} Bl
* @typedef {import('../interface').DecodeOptions} DecodeOptions
*/
/**
* @param {Uint8Array} data
* @param {number} offset
* @param {DecodeOptions} options
* @returns {number}
*/
function readUint8(data, offset, options) {
	assertEnoughData(data, offset, 1);
	const value = data[offset];
	if (options.strict === true && value < uintBoundaries[0]) throw new Error(`${decodeErrPrefix} integer encoded in more bytes than necessary (strict decode)`);
	return value;
}
/**
* @param {Uint8Array} data
* @param {number} offset
* @param {DecodeOptions} options
* @returns {number}
*/
function readUint16(data, offset, options) {
	assertEnoughData(data, offset, 2);
	const value = data[offset] << 8 | data[offset + 1];
	if (options.strict === true && value < uintBoundaries[1]) throw new Error(`${decodeErrPrefix} integer encoded in more bytes than necessary (strict decode)`);
	return value;
}
/**
* @param {Uint8Array} data
* @param {number} offset
* @param {DecodeOptions} options
* @returns {number}
*/
function readUint32(data, offset, options) {
	assertEnoughData(data, offset, 4);
	const value = data[offset] * 16777216 + (data[offset + 1] << 16) + (data[offset + 2] << 8) + data[offset + 3];
	if (options.strict === true && value < uintBoundaries[2]) throw new Error(`${decodeErrPrefix} integer encoded in more bytes than necessary (strict decode)`);
	return value;
}
/**
* @param {Uint8Array} data
* @param {number} offset
* @param {DecodeOptions} options
* @returns {number|bigint}
*/
function readUint64(data, offset, options) {
	assertEnoughData(data, offset, 8);
	const hi = data[offset] * 16777216 + (data[offset + 1] << 16) + (data[offset + 2] << 8) + data[offset + 3];
	const lo = data[offset + 4] * 16777216 + (data[offset + 5] << 16) + (data[offset + 6] << 8) + data[offset + 7];
	const value = (BigInt(hi) << BigInt(32)) + BigInt(lo);
	if (options.strict === true && value < uintBoundaries[3]) throw new Error(`${decodeErrPrefix} integer encoded in more bytes than necessary (strict decode)`);
	if (value <= Number.MAX_SAFE_INTEGER) return Number(value);
	if (options.allowBigInt === true) return value;
	throw new Error(`${decodeErrPrefix} integers outside of the safe integer range are not supported`);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeUint8(data, pos, _minor, options) {
	return new Token(Type$1.uint, readUint8(data, pos + 1, options), 2);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeUint16(data, pos, _minor, options) {
	return new Token(Type$1.uint, readUint16(data, pos + 1, options), 3);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeUint32(data, pos, _minor, options) {
	return new Token(Type$1.uint, readUint32(data, pos + 1, options), 5);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeUint64(data, pos, _minor, options) {
	return new Token(Type$1.uint, readUint64(data, pos + 1, options), 9);
}
/**
* @param {Bl} buf
* @param {Token} token
*/
function encodeUint(buf$1, token) {
	return encodeUintValue(buf$1, 0, token.value);
}
/**
* @param {Bl} buf
* @param {number} major
* @param {number|bigint} uint
*/
function encodeUintValue(buf$1, major, uint) {
	if (uint < uintBoundaries[0]) {
		const nuint = Number(uint);
		buf$1.push([major | nuint]);
	} else if (uint < uintBoundaries[1]) {
		const nuint = Number(uint);
		buf$1.push([major | 24, nuint]);
	} else if (uint < uintBoundaries[2]) {
		const nuint = Number(uint);
		buf$1.push([
			major | 25,
			nuint >>> 8,
			nuint & 255
		]);
	} else if (uint < uintBoundaries[3]) {
		const nuint = Number(uint);
		buf$1.push([
			major | 26,
			nuint >>> 24 & 255,
			nuint >>> 16 & 255,
			nuint >>> 8 & 255,
			nuint & 255
		]);
	} else {
		const buint = BigInt(uint);
		if (buint < uintBoundaries[4]) {
			const set = [
				major | 27,
				0,
				0,
				0,
				0,
				0,
				0,
				0
			];
			let lo = Number(buint & BigInt(4294967295));
			let hi = Number(buint >> BigInt(32) & BigInt(4294967295));
			set[8] = lo & 255;
			lo = lo >> 8;
			set[7] = lo & 255;
			lo = lo >> 8;
			set[6] = lo & 255;
			lo = lo >> 8;
			set[5] = lo & 255;
			set[4] = hi & 255;
			hi = hi >> 8;
			set[3] = hi & 255;
			hi = hi >> 8;
			set[2] = hi & 255;
			hi = hi >> 8;
			set[1] = hi & 255;
			buf$1.push(set);
		} else throw new Error(`${decodeErrPrefix} encountered BigInt larger than allowable range`);
	}
}
/**
* @param {Token} token
* @returns {number}
*/
encodeUint.encodedSize = function encodedSize(token) {
	return encodeUintValue.encodedSize(token.value);
};
/**
* @param {number} uint
* @returns {number}
*/
encodeUintValue.encodedSize = function encodedSize(uint) {
	if (uint < uintBoundaries[0]) return 1;
	if (uint < uintBoundaries[1]) return 2;
	if (uint < uintBoundaries[2]) return 3;
	if (uint < uintBoundaries[3]) return 5;
	return 9;
};
/**
* @param {Token} tok1
* @param {Token} tok2
* @returns {number}
*/
encodeUint.compareTokens = function compareTokens(tok1, tok2) {
	return tok1.value < tok2.value ? -1 : tok1.value > tok2.value ? 1 : 0;
};
/**
* @typedef {import('./bl.js').Bl} Bl
* @typedef {import('../interface').DecodeOptions} DecodeOptions
*/
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeNegint8(data, pos, _minor, options) {
	return new Token(Type$1.negint, -1 - readUint8(data, pos + 1, options), 2);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeNegint16(data, pos, _minor, options) {
	return new Token(Type$1.negint, -1 - readUint16(data, pos + 1, options), 3);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeNegint32(data, pos, _minor, options) {
	return new Token(Type$1.negint, -1 - readUint32(data, pos + 1, options), 5);
}
const neg1b = BigInt(-1);
const pos1b = BigInt(1);
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeNegint64(data, pos, _minor, options) {
	const int = readUint64(data, pos + 1, options);
	if (typeof int !== "bigint") {
		const value = -1 - int;
		if (value >= Number.MIN_SAFE_INTEGER) return new Token(Type$1.negint, value, 9);
	}
	if (options.allowBigInt !== true) throw new Error(`${decodeErrPrefix} integers outside of the safe integer range are not supported`);
	return new Token(Type$1.negint, neg1b - BigInt(int), 9);
}
/**
* @param {Bl} buf
* @param {Token} token
*/
function encodeNegint(buf$1, token) {
	const negint = token.value;
	const unsigned = typeof negint === "bigint" ? negint * neg1b - pos1b : negint * -1 - 1;
	encodeUintValue(buf$1, token.type.majorEncoded, unsigned);
}
/**
* @param {Token} token
* @returns {number}
*/
encodeNegint.encodedSize = function encodedSize(token) {
	const negint = token.value;
	const unsigned = typeof negint === "bigint" ? negint * neg1b - pos1b : negint * -1 - 1;
	if (unsigned < uintBoundaries[0]) return 1;
	if (unsigned < uintBoundaries[1]) return 2;
	if (unsigned < uintBoundaries[2]) return 3;
	if (unsigned < uintBoundaries[3]) return 5;
	return 9;
};
/**
* @param {Token} tok1
* @param {Token} tok2
* @returns {number}
*/
encodeNegint.compareTokens = function compareTokens(tok1, tok2) {
	return tok1.value < tok2.value ? 1 : tok1.value > tok2.value ? -1 : 0;
};
/**
* @typedef {import('./bl.js').Bl} Bl
* @typedef {import('../interface').DecodeOptions} DecodeOptions
*/
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} prefix
* @param {number} length
* @returns {Token}
*/
function toToken$3(data, pos, prefix, length) {
	assertEnoughData(data, pos, prefix + length);
	const buf$1 = slice(data, pos + prefix, pos + prefix + length);
	return new Token(Type$1.bytes, buf$1, prefix + length);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} minor
* @param {DecodeOptions} _options
* @returns {Token}
*/
function decodeBytesCompact(data, pos, minor, _options) {
	return toToken$3(data, pos, 1, minor);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeBytes8(data, pos, _minor, options) {
	return toToken$3(data, pos, 2, readUint8(data, pos + 1, options));
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeBytes16(data, pos, _minor, options) {
	return toToken$3(data, pos, 3, readUint16(data, pos + 1, options));
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeBytes32(data, pos, _minor, options) {
	return toToken$3(data, pos, 5, readUint32(data, pos + 1, options));
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeBytes64(data, pos, _minor, options) {
	const l = readUint64(data, pos + 1, options);
	if (typeof l === "bigint") throw new Error(`${decodeErrPrefix} 64-bit integer bytes lengths not supported`);
	return toToken$3(data, pos, 9, l);
}
/**
* `encodedBytes` allows for caching when we do a byte version of a string
* for key sorting purposes
* @param {Token} token
* @returns {Uint8Array}
*/
function tokenBytes(token) {
	if (token.encodedBytes === undefined) token.encodedBytes = token.type === Type$1.string ? fromString(token.value) : token.value;
	return token.encodedBytes;
}
/**
* @param {Bl} buf
* @param {Token} token
*/
function encodeBytes(buf$1, token) {
	const bytes = tokenBytes(token);
	encodeUintValue(buf$1, token.type.majorEncoded, bytes.length);
	buf$1.push(bytes);
}
/**
* @param {Token} token
* @returns {number}
*/
encodeBytes.encodedSize = function encodedSize(token) {
	const bytes = tokenBytes(token);
	return encodeUintValue.encodedSize(bytes.length) + bytes.length;
};
/**
* @param {Token} tok1
* @param {Token} tok2
* @returns {number}
*/
encodeBytes.compareTokens = function compareTokens(tok1, tok2) {
	return compareBytes(tokenBytes(tok1), tokenBytes(tok2));
};
/**
* @param {Uint8Array} b1
* @param {Uint8Array} b2
* @returns {number}
*/
function compareBytes(b1, b2) {
	return b1.length < b2.length ? -1 : b1.length > b2.length ? 1 : compare(b1, b2);
}
/**
* @typedef {import('./bl.js').Bl} Bl
* @typedef {import('../interface').DecodeOptions} DecodeOptions
*/
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} prefix
* @param {number} length
* @param {DecodeOptions} options
* @returns {Token}
*/
function toToken$2(data, pos, prefix, length, options) {
	const totLength = prefix + length;
	assertEnoughData(data, pos, totLength);
	const tok = new Token(Type$1.string, toString(data, pos + prefix, pos + totLength), totLength);
	if (options.retainStringBytes === true) tok.byteValue = slice(data, pos + prefix, pos + totLength);
	return tok;
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeStringCompact(data, pos, minor, options) {
	return toToken$2(data, pos, 1, minor, options);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeString8(data, pos, _minor, options) {
	return toToken$2(data, pos, 2, readUint8(data, pos + 1, options), options);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeString16(data, pos, _minor, options) {
	return toToken$2(data, pos, 3, readUint16(data, pos + 1, options), options);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeString32(data, pos, _minor, options) {
	return toToken$2(data, pos, 5, readUint32(data, pos + 1, options), options);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeString64(data, pos, _minor, options) {
	const l = readUint64(data, pos + 1, options);
	if (typeof l === "bigint") throw new Error(`${decodeErrPrefix} 64-bit integer string lengths not supported`);
	return toToken$2(data, pos, 9, l, options);
}
const encodeString = encodeBytes;
/**
* @typedef {import('./bl.js').Bl} Bl
* @typedef {import('../interface').DecodeOptions} DecodeOptions
*/
/**
* @param {Uint8Array} _data
* @param {number} _pos
* @param {number} prefix
* @param {number} length
* @returns {Token}
*/
function toToken$1(_data, _pos, prefix, length) {
	return new Token(Type$1.array, length, prefix);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} minor
* @param {DecodeOptions} _options
* @returns {Token}
*/
function decodeArrayCompact(data, pos, minor, _options) {
	return toToken$1(data, pos, 1, minor);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeArray8(data, pos, _minor, options) {
	return toToken$1(data, pos, 2, readUint8(data, pos + 1, options));
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeArray16(data, pos, _minor, options) {
	return toToken$1(data, pos, 3, readUint16(data, pos + 1, options));
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeArray32(data, pos, _minor, options) {
	return toToken$1(data, pos, 5, readUint32(data, pos + 1, options));
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeArray64(data, pos, _minor, options) {
	const l = readUint64(data, pos + 1, options);
	if (typeof l === "bigint") throw new Error(`${decodeErrPrefix} 64-bit integer array lengths not supported`);
	return toToken$1(data, pos, 9, l);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeArrayIndefinite(data, pos, _minor, options) {
	if (options.allowIndefinite === false) throw new Error(`${decodeErrPrefix} indefinite length items not allowed`);
	return toToken$1(data, pos, 1, Infinity);
}
/**
* @param {Bl} buf
* @param {Token} token
*/
function encodeArray(buf$1, token) {
	encodeUintValue(buf$1, Type$1.array.majorEncoded, token.value);
}
encodeArray.compareTokens = encodeUint.compareTokens;
/**
* @param {Token} token
* @returns {number}
*/
encodeArray.encodedSize = function encodedSize(token) {
	return encodeUintValue.encodedSize(token.value);
};
/**
* @typedef {import('./bl.js').Bl} Bl
* @typedef {import('../interface').DecodeOptions} DecodeOptions
*/
/**
* @param {Uint8Array} _data
* @param {number} _pos
* @param {number} prefix
* @param {number} length
* @returns {Token}
*/
function toToken(_data, _pos, prefix, length) {
	return new Token(Type$1.map, length, prefix);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} minor
* @param {DecodeOptions} _options
* @returns {Token}
*/
function decodeMapCompact(data, pos, minor, _options) {
	return toToken(data, pos, 1, minor);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeMap8(data, pos, _minor, options) {
	return toToken(data, pos, 2, readUint8(data, pos + 1, options));
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeMap16(data, pos, _minor, options) {
	return toToken(data, pos, 3, readUint16(data, pos + 1, options));
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeMap32(data, pos, _minor, options) {
	return toToken(data, pos, 5, readUint32(data, pos + 1, options));
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeMap64(data, pos, _minor, options) {
	const l = readUint64(data, pos + 1, options);
	if (typeof l === "bigint") throw new Error(`${decodeErrPrefix} 64-bit integer map lengths not supported`);
	return toToken(data, pos, 9, l);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeMapIndefinite(data, pos, _minor, options) {
	if (options.allowIndefinite === false) throw new Error(`${decodeErrPrefix} indefinite length items not allowed`);
	return toToken(data, pos, 1, Infinity);
}
/**
* @param {Bl} buf
* @param {Token} token
*/
function encodeMap(buf$1, token) {
	encodeUintValue(buf$1, Type$1.map.majorEncoded, token.value);
}
encodeMap.compareTokens = encodeUint.compareTokens;
/**
* @param {Token} token
* @returns {number}
*/
encodeMap.encodedSize = function encodedSize(token) {
	return encodeUintValue.encodedSize(token.value);
};
/**
* @typedef {import('./bl.js').Bl} Bl
* @typedef {import('../interface').DecodeOptions} DecodeOptions
*/
/**
* @param {Uint8Array} _data
* @param {number} _pos
* @param {number} minor
* @param {DecodeOptions} _options
* @returns {Token}
*/
function decodeTagCompact(_data, _pos, minor, _options) {
	return new Token(Type$1.tag, minor, 1);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeTag8(data, pos, _minor, options) {
	return new Token(Type$1.tag, readUint8(data, pos + 1, options), 2);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeTag16(data, pos, _minor, options) {
	return new Token(Type$1.tag, readUint16(data, pos + 1, options), 3);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeTag32(data, pos, _minor, options) {
	return new Token(Type$1.tag, readUint32(data, pos + 1, options), 5);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeTag64(data, pos, _minor, options) {
	return new Token(Type$1.tag, readUint64(data, pos + 1, options), 9);
}
/**
* @param {Bl} buf
* @param {Token} token
*/
function encodeTag(buf$1, token) {
	encodeUintValue(buf$1, Type$1.tag.majorEncoded, token.value);
}
encodeTag.compareTokens = encodeUint.compareTokens;
/**
* @param {Token} token
* @returns {number}
*/
encodeTag.encodedSize = function encodedSize(token) {
	return encodeUintValue.encodedSize(token.value);
};
/**
* @typedef {import('./bl.js').Bl} Bl
* @typedef {import('../interface').DecodeOptions} DecodeOptions
* @typedef {import('../interface').EncodeOptions} EncodeOptions
*/
const MINOR_FALSE = 20;
const MINOR_TRUE = 21;
const MINOR_NULL = 22;
const MINOR_UNDEFINED = 23;
/**
* @param {Uint8Array} _data
* @param {number} _pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeUndefined(_data, _pos, _minor, options) {
	if (options.allowUndefined === false) throw new Error(`${decodeErrPrefix} undefined values are not supported`);
else if (options.coerceUndefinedToNull === true) return new Token(Type$1.null, null, 1);
	return new Token(Type$1.undefined, undefined, 1);
}
/**
* @param {Uint8Array} _data
* @param {number} _pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeBreak(_data, _pos, _minor, options) {
	if (options.allowIndefinite === false) throw new Error(`${decodeErrPrefix} indefinite length items not allowed`);
	return new Token(Type$1.break, undefined, 1);
}
/**
* @param {number} value
* @param {number} bytes
* @param {DecodeOptions} options
* @returns {Token}
*/
function createToken(value, bytes, options) {
	if (options) {
		if (options.allowNaN === false && Number.isNaN(value)) throw new Error(`${decodeErrPrefix} NaN values are not supported`);
		if (options.allowInfinity === false && (value === Infinity || value === -Infinity)) throw new Error(`${decodeErrPrefix} Infinity values are not supported`);
	}
	return new Token(Type$1.float, value, bytes);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeFloat16(data, pos, _minor, options) {
	return createToken(readFloat16(data, pos + 1), 3, options);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeFloat32(data, pos, _minor, options) {
	return createToken(readFloat32(data, pos + 1), 5, options);
}
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} _minor
* @param {DecodeOptions} options
* @returns {Token}
*/
function decodeFloat64(data, pos, _minor, options) {
	return createToken(readFloat64(data, pos + 1), 9, options);
}
/**
* @param {Bl} buf
* @param {Token} token
* @param {EncodeOptions} options
*/
function encodeFloat(buf$1, token, options) {
	const float = token.value;
	if (float === false) buf$1.push([Type$1.float.majorEncoded | MINOR_FALSE]);
else if (float === true) buf$1.push([Type$1.float.majorEncoded | MINOR_TRUE]);
else if (float === null) buf$1.push([Type$1.float.majorEncoded | MINOR_NULL]);
else if (float === undefined) buf$1.push([Type$1.float.majorEncoded | MINOR_UNDEFINED]);
else {
		let decoded;
		let success = false;
		if (!options || options.float64 !== true) {
			encodeFloat16(float);
			decoded = readFloat16(ui8a, 1);
			if (float === decoded || Number.isNaN(float)) {
				ui8a[0] = 249;
				buf$1.push(ui8a.slice(0, 3));
				success = true;
			} else {
				encodeFloat32(float);
				decoded = readFloat32(ui8a, 1);
				if (float === decoded) {
					ui8a[0] = 250;
					buf$1.push(ui8a.slice(0, 5));
					success = true;
				}
			}
		}
		if (!success) {
			encodeFloat64(float);
			decoded = readFloat64(ui8a, 1);
			ui8a[0] = 251;
			buf$1.push(ui8a.slice(0, 9));
		}
	}
}
/**
* @param {Token} token
* @param {EncodeOptions} options
* @returns {number}
*/
encodeFloat.encodedSize = function encodedSize(token, options) {
	const float = token.value;
	if (float === false || float === true || float === null || float === undefined) return 1;
	if (!options || options.float64 !== true) {
		encodeFloat16(float);
		let decoded = readFloat16(ui8a, 1);
		if (float === decoded || Number.isNaN(float)) return 3;
		encodeFloat32(float);
		decoded = readFloat32(ui8a, 1);
		if (float === decoded) return 5;
	}
	return 9;
};
const buffer = new ArrayBuffer(9);
const dataView = new DataView(buffer, 1);
const ui8a = new Uint8Array(buffer, 0);
/**
* @param {number} inp
*/
function encodeFloat16(inp) {
	if (inp === Infinity) dataView.setUint16(0, 31744, false);
else if (inp === -Infinity) dataView.setUint16(0, 64512, false);
else if (Number.isNaN(inp)) dataView.setUint16(0, 32256, false);
else {
		dataView.setFloat32(0, inp);
		const valu32 = dataView.getUint32(0);
		const exponent = (valu32 & 2139095040) >> 23;
		const mantissa = valu32 & 8388607;
		if (exponent === 255) dataView.setUint16(0, 31744, false);
else if (exponent === 0) dataView.setUint16(0, (inp & 2147483648) >> 16 | mantissa >> 13, false);
else {
			const logicalExponent = exponent - 127;
			if (logicalExponent < -24) dataView.setUint16(0, 0);
else if (logicalExponent < -14) dataView.setUint16(0, (valu32 & 2147483648) >> 16 | 1 << 24 + logicalExponent, false);
else dataView.setUint16(0, (valu32 & 2147483648) >> 16 | logicalExponent + 15 << 10 | mantissa >> 13, false);
		}
	}
}
/**
* @param {Uint8Array} ui8a
* @param {number} pos
* @returns {number}
*/
function readFloat16(ui8a$1, pos) {
	if (ui8a$1.length - pos < 2) throw new Error(`${decodeErrPrefix} not enough data for float16`);
	const half = (ui8a$1[pos] << 8) + ui8a$1[pos + 1];
	if (half === 31744) return Infinity;
	if (half === 64512) return -Infinity;
	if (half === 32256) return NaN;
	const exp = half >> 10 & 31;
	const mant = half & 1023;
	let val;
	if (exp === 0) val = mant * 5.960464477539063e-8;
else if (exp !== 31) val = (mant + 1024) * 2 ** (exp - 25);
else val = mant === 0 ? Infinity : NaN;
	return half & 32768 ? -val : val;
}
/**
* @param {number} inp
*/
function encodeFloat32(inp) {
	dataView.setFloat32(0, inp, false);
}
/**
* @param {Uint8Array} ui8a
* @param {number} pos
* @returns {number}
*/
function readFloat32(ui8a$1, pos) {
	if (ui8a$1.length - pos < 4) throw new Error(`${decodeErrPrefix} not enough data for float32`);
	const offset = (ui8a$1.byteOffset || 0) + pos;
	return new DataView(ui8a$1.buffer, offset, 4).getFloat32(0, false);
}
/**
* @param {number} inp
*/
function encodeFloat64(inp) {
	dataView.setFloat64(0, inp, false);
}
/**
* @param {Uint8Array} ui8a
* @param {number} pos
* @returns {number}
*/
function readFloat64(ui8a$1, pos) {
	if (ui8a$1.length - pos < 8) throw new Error(`${decodeErrPrefix} not enough data for float64`);
	const offset = (ui8a$1.byteOffset || 0) + pos;
	return new DataView(ui8a$1.buffer, offset, 8).getFloat64(0, false);
}
/**
* @param {Token} _tok1
* @param {Token} _tok2
* @returns {number}
*/
encodeFloat.compareTokens = encodeUint.compareTokens;
/**
* @typedef {import('../interface').DecodeOptions} DecodeOptions
*/
/**
* @param {Uint8Array} data
* @param {number} pos
* @param {number} minor
*/
function invalidMinor(data, pos, minor) {
	throw new Error(`${decodeErrPrefix} encountered invalid minor (${minor}) for major ${data[pos] >>> 5}`);
}
/**
* @param {string} msg
* @returns {()=>any}
*/
function errorer(msg) {
	return () => {
		throw new Error(`${decodeErrPrefix} ${msg}`);
	};
}
/** @type {((data:Uint8Array, pos:number, minor:number, options?:DecodeOptions) => any)[]} */
const jump = [];
for (let i = 0; i <= 23; i++) jump[i] = invalidMinor;
jump[24] = decodeUint8;
jump[25] = decodeUint16;
jump[26] = decodeUint32;
jump[27] = decodeUint64;
jump[28] = invalidMinor;
jump[29] = invalidMinor;
jump[30] = invalidMinor;
jump[31] = invalidMinor;
for (let i = 32; i <= 55; i++) jump[i] = invalidMinor;
jump[56] = decodeNegint8;
jump[57] = decodeNegint16;
jump[58] = decodeNegint32;
jump[59] = decodeNegint64;
jump[60] = invalidMinor;
jump[61] = invalidMinor;
jump[62] = invalidMinor;
jump[63] = invalidMinor;
for (let i = 64; i <= 87; i++) jump[i] = decodeBytesCompact;
jump[88] = decodeBytes8;
jump[89] = decodeBytes16;
jump[90] = decodeBytes32;
jump[91] = decodeBytes64;
jump[92] = invalidMinor;
jump[93] = invalidMinor;
jump[94] = invalidMinor;
jump[95] = errorer("indefinite length bytes/strings are not supported");
for (let i = 96; i <= 119; i++) jump[i] = decodeStringCompact;
jump[120] = decodeString8;
jump[121] = decodeString16;
jump[122] = decodeString32;
jump[123] = decodeString64;
jump[124] = invalidMinor;
jump[125] = invalidMinor;
jump[126] = invalidMinor;
jump[127] = errorer("indefinite length bytes/strings are not supported");
for (let i = 128; i <= 151; i++) jump[i] = decodeArrayCompact;
jump[152] = decodeArray8;
jump[153] = decodeArray16;
jump[154] = decodeArray32;
jump[155] = decodeArray64;
jump[156] = invalidMinor;
jump[157] = invalidMinor;
jump[158] = invalidMinor;
jump[159] = decodeArrayIndefinite;
for (let i = 160; i <= 183; i++) jump[i] = decodeMapCompact;
jump[184] = decodeMap8;
jump[185] = decodeMap16;
jump[186] = decodeMap32;
jump[187] = decodeMap64;
jump[188] = invalidMinor;
jump[189] = invalidMinor;
jump[190] = invalidMinor;
jump[191] = decodeMapIndefinite;
for (let i = 192; i <= 215; i++) jump[i] = decodeTagCompact;
jump[216] = decodeTag8;
jump[217] = decodeTag16;
jump[218] = decodeTag32;
jump[219] = decodeTag64;
jump[220] = invalidMinor;
jump[221] = invalidMinor;
jump[222] = invalidMinor;
jump[223] = invalidMinor;
for (let i = 224; i <= 243; i++) jump[i] = errorer("simple values are not supported");
jump[244] = invalidMinor;
jump[245] = invalidMinor;
jump[246] = invalidMinor;
jump[247] = decodeUndefined;
jump[248] = errorer("simple values are not supported");
jump[249] = decodeFloat16;
jump[250] = decodeFloat32;
jump[251] = decodeFloat64;
jump[252] = invalidMinor;
jump[253] = invalidMinor;
jump[254] = invalidMinor;
jump[255] = decodeBreak;
/** @type {Token[]} */
const quick = [];
for (let i = 0; i < 24; i++) quick[i] = new Token(Type$1.uint, i, 1);
for (let i = -1; i >= -24; i--) quick[31 - i] = new Token(Type$1.negint, i, 1);
quick[64] = new Token(Type$1.bytes, new Uint8Array(0), 1);
quick[96] = new Token(Type$1.string, "", 1);
quick[128] = new Token(Type$1.array, 0, 1);
quick[160] = new Token(Type$1.map, 0, 1);
quick[244] = new Token(Type$1.false, false, 1);
quick[245] = new Token(Type$1.true, true, 1);
quick[246] = new Token(Type$1.null, null, 1);
/**
* @param {Token} token
* @returns {Uint8Array|undefined}
*/
function quickEncodeToken(token) {
	switch (token.type) {
		case Type$1.false: return fromArray([244]);
		case Type$1.true: return fromArray([245]);
		case Type$1.null: return fromArray([246]);
		case Type$1.bytes:
			if (!token.value.length) return fromArray([64]);
			return;
		case Type$1.string:
			if (token.value === "") return fromArray([96]);
			return;
		case Type$1.array:
			if (token.value === 0) return fromArray([128]);
			return;
		case Type$1.map:
			if (token.value === 0) return fromArray([160]);
			return;
		case Type$1.uint:
			if (token.value < 24) return fromArray([Number(token.value)]);
			return;
		case Type$1.negint: if (token.value >= -24) return fromArray([31 - Number(token.value)]);
	}
}
/**
* @typedef {import('../interface').EncodeOptions} EncodeOptions
* @typedef {import('../interface').OptionalTypeEncoder} OptionalTypeEncoder
* @typedef {import('../interface').Reference} Reference
* @typedef {import('../interface').StrictTypeEncoder} StrictTypeEncoder
* @typedef {import('../interface').TokenTypeEncoder} TokenTypeEncoder
* @typedef {import('../interface').TokenOrNestedTokens} TokenOrNestedTokens
*/
/** @type {EncodeOptions} */
const defaultEncodeOptions = {
	float64: false,
	mapSorter,
	quickEncodeToken
};
/** @returns {TokenTypeEncoder[]} */
function makeCborEncoders() {
	const encoders = [];
	encoders[Type$1.uint.major] = encodeUint;
	encoders[Type$1.negint.major] = encodeNegint;
	encoders[Type$1.bytes.major] = encodeBytes;
	encoders[Type$1.string.major] = encodeString;
	encoders[Type$1.array.major] = encodeArray;
	encoders[Type$1.map.major] = encodeMap;
	encoders[Type$1.tag.major] = encodeTag;
	encoders[Type$1.float.major] = encodeFloat;
	return encoders;
}
const cborEncoders = makeCborEncoders();
const buf = new Bl();
var Ref = class Ref {
	/**
	* @param {object|any[]} obj
	* @param {Reference|undefined} parent
	*/
	constructor(obj, parent) {
		this.obj = obj;
		this.parent = parent;
	}
	/**
	* @param {object|any[]} obj
	* @returns {boolean}
	*/
	includes(obj) {
		/** @type {Reference|undefined} */
		let p = this;
		do 
			if (p.obj === obj) return true;
		while (p = p.parent);
		return false;
	}
	/**
	* @param {Reference|undefined} stack
	* @param {object|any[]} obj
	* @returns {Reference}
	*/
	static createCheck(stack, obj) {
		if (stack && stack.includes(obj)) throw new Error(`${encodeErrPrefix} object contains circular references`);
		return new Ref(obj, stack);
	}
};
const simpleTokens = {
	null: new Token(Type$1.null, null),
	undefined: new Token(Type$1.undefined, undefined),
	true: new Token(Type$1.true, true),
	false: new Token(Type$1.false, false),
	emptyArray: new Token(Type$1.array, 0),
	emptyMap: new Token(Type$1.map, 0)
};
/** @type {{[typeName: string]: StrictTypeEncoder}} */
const typeEncoders = {
	number(obj, _typ, _options, _refStack) {
		if (!Number.isInteger(obj) || !Number.isSafeInteger(obj)) return new Token(Type$1.float, obj);
else if (obj >= 0) return new Token(Type$1.uint, obj);
else return new Token(Type$1.negint, obj);
	},
	bigint(obj, _typ, _options, _refStack) {
		if (obj >= BigInt(0)) return new Token(Type$1.uint, obj);
else return new Token(Type$1.negint, obj);
	},
	Uint8Array(obj, _typ, _options, _refStack) {
		return new Token(Type$1.bytes, obj);
	},
	string(obj, _typ, _options, _refStack) {
		return new Token(Type$1.string, obj);
	},
	boolean(obj, _typ, _options, _refStack) {
		return obj ? simpleTokens.true : simpleTokens.false;
	},
	null(_obj, _typ, _options, _refStack) {
		return simpleTokens.null;
	},
	undefined(_obj, _typ, _options, _refStack) {
		return simpleTokens.undefined;
	},
	ArrayBuffer(obj, _typ, _options, _refStack) {
		return new Token(Type$1.bytes, new Uint8Array(obj));
	},
	DataView(obj, _typ, _options, _refStack) {
		return new Token(Type$1.bytes, new Uint8Array(obj.buffer, obj.byteOffset, obj.byteLength));
	},
	Array(obj, _typ, options, refStack) {
		if (!obj.length) {
			if (options.addBreakTokens === true) return [simpleTokens.emptyArray, new Token(Type$1.break)];
			return simpleTokens.emptyArray;
		}
		refStack = Ref.createCheck(refStack, obj);
		const entries = [];
		let i = 0;
		for (const e of obj) entries[i++] = objectToTokens(e, options, refStack);
		if (options.addBreakTokens) return [
			new Token(Type$1.array, obj.length),
			entries,
			new Token(Type$1.break)
		];
		return [new Token(Type$1.array, obj.length), entries];
	},
	Object(obj, typ, options, refStack) {
		const isMap = typ !== "Object";
		const keys = isMap ? obj.keys() : Object.keys(obj);
		const length = isMap ? obj.size : keys.length;
		if (!length) {
			if (options.addBreakTokens === true) return [simpleTokens.emptyMap, new Token(Type$1.break)];
			return simpleTokens.emptyMap;
		}
		refStack = Ref.createCheck(refStack, obj);
		/** @type {TokenOrNestedTokens[]} */
		const entries = [];
		let i = 0;
		for (const key of keys) entries[i++] = [objectToTokens(key, options, refStack), objectToTokens(isMap ? obj.get(key) : obj[key], options, refStack)];
		sortMapEntries(entries, options);
		if (options.addBreakTokens) return [
			new Token(Type$1.map, length),
			entries,
			new Token(Type$1.break)
		];
		return [new Token(Type$1.map, length), entries];
	}
};
typeEncoders.Map = typeEncoders.Object;
typeEncoders.Buffer = typeEncoders.Uint8Array;
for (const typ of "Uint8Clamped Uint16 Uint32 Int8 Int16 Int32 BigUint64 BigInt64 Float32 Float64".split(" ")) typeEncoders[`${typ}Array`] = typeEncoders.DataView;
/**
* @param {any} obj
* @param {EncodeOptions} [options]
* @param {Reference} [refStack]
* @returns {TokenOrNestedTokens}
*/
function objectToTokens(obj, options = {}, refStack) {
	const typ = is(obj);
	const customTypeEncoder = options && options.typeEncoders && options.typeEncoders[typ] || typeEncoders[typ];
	if (typeof customTypeEncoder === "function") {
		const tokens = customTypeEncoder(obj, typ, options, refStack);
		if (tokens != null) return tokens;
	}
	const typeEncoder = typeEncoders[typ];
	if (!typeEncoder) throw new Error(`${encodeErrPrefix} unsupported type: ${typ}`);
	return typeEncoder(obj, typ, options, refStack);
}
/**
* @param {TokenOrNestedTokens[]} entries
* @param {EncodeOptions} options
*/
function sortMapEntries(entries, options) {
	if (options.mapSorter) entries.sort(options.mapSorter);
}
/**
* @param {(Token|Token[])[]} e1
* @param {(Token|Token[])[]} e2
* @returns {number}
*/
function mapSorter(e1, e2) {
	const keyToken1 = Array.isArray(e1[0]) ? e1[0][0] : e1[0];
	const keyToken2 = Array.isArray(e2[0]) ? e2[0][0] : e2[0];
	if (keyToken1.type !== keyToken2.type) return keyToken1.type.compare(keyToken2.type);
	const major = keyToken1.type.major;
	const tcmp = cborEncoders[major].compareTokens(keyToken1, keyToken2);
	if (tcmp === 0) console.warn("WARNING: complex key types used, CBOR key sorting guarantees are gone");
	return tcmp;
}
/**
* @param {Bl} buf
* @param {TokenOrNestedTokens} tokens
* @param {TokenTypeEncoder[]} encoders
* @param {EncodeOptions} options
*/
function tokensToEncoded(buf$1, tokens, encoders, options) {
	if (Array.isArray(tokens)) for (const token of tokens) tokensToEncoded(buf$1, token, encoders, options);
else encoders[tokens.type.major](buf$1, tokens, options);
}
/**
* @param {any} data
* @param {TokenTypeEncoder[]} encoders
* @param {EncodeOptions} options
* @returns {Uint8Array}
*/
function encodeCustom(data, encoders, options) {
	const tokens = objectToTokens(data, options);
	if (!Array.isArray(tokens) && options.quickEncodeToken) {
		const quickBytes = options.quickEncodeToken(tokens);
		if (quickBytes) return quickBytes;
		const encoder = encoders[tokens.type.major];
		if (encoder.encodedSize) {
			const size = encoder.encodedSize(tokens, options);
			const buf$1 = new Bl(size);
			encoder(buf$1, tokens, options);
			if (buf$1.chunks.length !== 1) throw new Error(`Unexpected error: pre-calculated length for ${tokens} was wrong`);
			return asU8A(buf$1.chunks[0]);
		}
	}
	buf.reset();
	tokensToEncoded(buf, tokens, encoders, options);
	return buf.toBytes(true);
}
/**
* @param {any} data
* @param {EncodeOptions} [options]
* @returns {Uint8Array}
*/
function encode(data, options) {
	options = Object.assign({}, defaultEncodeOptions, options);
	return encodeCustom(data, cborEncoders, options);
}
/**
* @typedef {import('./token.js').Token} Token
* @typedef {import('../interface').DecodeOptions} DecodeOptions
* @typedef {import('../interface').DecodeTokenizer} DecodeTokenizer
*/
const defaultDecodeOptions = {
	strict: false,
	allowIndefinite: true,
	allowUndefined: true,
	allowBigInt: true
};
var Tokeniser = class {
	/**
	* @param {Uint8Array} data
	* @param {DecodeOptions} options
	*/
	constructor(data, options = {}) {
		this._pos = 0;
		this.data = data;
		this.options = options;
	}
	pos() {
		return this._pos;
	}
	done() {
		return this._pos >= this.data.length;
	}
	next() {
		const byt = this.data[this._pos];
		let token = quick[byt];
		if (token === undefined) {
			const decoder = jump[byt];
			if (!decoder) throw new Error(`${decodeErrPrefix} no decoder for major type ${byt >>> 5} (byte 0x${byt.toString(16).padStart(2, "0")})`);
			const minor = byt & 31;
			token = decoder(this.data, this._pos, minor, this.options);
		}
		this._pos += token.encodedLength;
		return token;
	}
};
const DONE = Symbol.for("DONE");
const BREAK = Symbol.for("BREAK");
/**
* @param {Token} token
* @param {DecodeTokenizer} tokeniser
* @param {DecodeOptions} options
* @returns {any|BREAK|DONE}
*/
function tokenToArray(token, tokeniser, options) {
	const arr = [];
	for (let i = 0; i < token.value; i++) {
		const value = tokensToObject(tokeniser, options);
		if (value === BREAK) {
			if (token.value === Infinity) break;
			throw new Error(`${decodeErrPrefix} got unexpected break to lengthed array`);
		}
		if (value === DONE) throw new Error(`${decodeErrPrefix} found array but not enough entries (got ${i}, expected ${token.value})`);
		arr[i] = value;
	}
	return arr;
}
/**
* @param {Token} token
* @param {DecodeTokenizer} tokeniser
* @param {DecodeOptions} options
* @returns {any|BREAK|DONE}
*/
function tokenToMap(token, tokeniser, options) {
	const useMaps = options.useMaps === true;
	const obj = useMaps ? undefined : {};
	const m = useMaps ? new Map() : undefined;
	for (let i = 0; i < token.value; i++) {
		const key = tokensToObject(tokeniser, options);
		if (key === BREAK) {
			if (token.value === Infinity) break;
			throw new Error(`${decodeErrPrefix} got unexpected break to lengthed map`);
		}
		if (key === DONE) throw new Error(`${decodeErrPrefix} found map but not enough entries (got ${i} [no key], expected ${token.value})`);
		if (useMaps !== true && typeof key !== "string") throw new Error(`${decodeErrPrefix} non-string keys not supported (got ${typeof key})`);
		if (options.rejectDuplicateMapKeys === true) {
			if (useMaps && m.has(key) || !useMaps && key in obj) throw new Error(`${decodeErrPrefix} found repeat map key "${key}"`);
		}
		const value = tokensToObject(tokeniser, options);
		if (value === DONE) throw new Error(`${decodeErrPrefix} found map but not enough entries (got ${i} [no value], expected ${token.value})`);
		if (useMaps) m.set(key, value);
else obj[key] = value;
	}
	return useMaps ? m : obj;
}
/**
* @param {DecodeTokenizer} tokeniser
* @param {DecodeOptions} options
* @returns {any|BREAK|DONE}
*/
function tokensToObject(tokeniser, options) {
	if (tokeniser.done()) return DONE;
	const token = tokeniser.next();
	if (token.type === Type$1.break) return BREAK;
	if (token.type.terminal) return token.value;
	if (token.type === Type$1.array) return tokenToArray(token, tokeniser, options);
	if (token.type === Type$1.map) return tokenToMap(token, tokeniser, options);
	if (token.type === Type$1.tag) {
		if (options.tags && typeof options.tags[token.value] === "function") {
			const tagged = tokensToObject(tokeniser, options);
			return options.tags[token.value](tagged);
		}
		throw new Error(`${decodeErrPrefix} tag not supported (${token.value})`);
	}
	throw new Error("unsupported");
}
/**
* @param {Uint8Array} data
* @param {DecodeOptions} [options]
* @returns {[any, Uint8Array]}
*/
function decodeFirst(data, options) {
	if (!(data instanceof Uint8Array)) throw new Error(`${decodeErrPrefix} data to decode must be a Uint8Array`);
	options = Object.assign({}, defaultDecodeOptions, options);
	const tokeniser = options.tokenizer || new Tokeniser(data, options);
	const decoded = tokensToObject(tokeniser, options);
	if (decoded === DONE) throw new Error(`${decodeErrPrefix} did not find any content to decode`);
	if (decoded === BREAK) throw new Error(`${decodeErrPrefix} got unexpected break`);
	return [decoded, data.subarray(tokeniser.pos())];
}
/**
* @param {Uint8Array} data
* @param {DecodeOptions} [options]
* @returns {any}
*/
function decode(data, options) {
	const [decoded, remainder] = decodeFirst(data, options);
	if (remainder.length > 0) throw new Error(`${decodeErrPrefix} too many terminals, data makes no sense`);
	return decoded;
}

//#endregion
//#region ../src/common/api/worker/rest/CustomCacheHandler.ts
var CustomCacheHandlerMap = class {
	handlers;
	constructor(...args) {
		const handlers = new Map();
		for (const { ref, handler } of args) {
			const key = getTypeId(ref);
			handlers.set(key, handler);
		}
		this.handlers = freezeMap(handlers);
	}
	get(typeRef) {
		const typeId = getTypeId(typeRef);
		return this.handlers.get(typeId);
	}
};
var CustomCalendarEventCacheHandler = class {
	constructor(entityRestClient) {
		this.entityRestClient = entityRestClient;
	}
	async loadRange(storage, listId, start, count, reverse) {
		const range = await storage.getRangeForList(CalendarEventTypeRef, listId);
		let rawList = [];
		if (range == null) {
			let chunk = [];
			let currentMin = CUSTOM_MIN_ID;
			while (true) {
				chunk = await this.entityRestClient.loadRange(CalendarEventTypeRef, listId, currentMin, LOAD_MULTIPLE_LIMIT, false);
				rawList.push(...chunk);
				if (chunk.length < LOAD_MULTIPLE_LIMIT) break;
				currentMin = getElementId(chunk[chunk.length - 1]);
			}
			for (const event of rawList) await storage.put(event);
			await storage.setNewRangeForList(CalendarEventTypeRef, listId, CUSTOM_MIN_ID, CUSTOM_MAX_ID);
		} else {
			this.assertCorrectRange(range);
			rawList = await storage.getWholeList(CalendarEventTypeRef, listId);
			console.log(`CalendarEvent list ${listId} has ${rawList.length} events`);
		}
		const typeModel = await resolveTypeReference(CalendarEventTypeRef);
		const sortedList = reverse ? rawList.filter((calendarEvent) => firstBiggerThanSecond(start, getElementId(calendarEvent), typeModel)).sort((a, b) => firstBiggerThanSecond(getElementId(b), getElementId(a), typeModel) ? 1 : -1) : rawList.filter((calendarEvent) => firstBiggerThanSecond(getElementId(calendarEvent), start, typeModel)).sort((a, b) => firstBiggerThanSecond(getElementId(a), getElementId(b), typeModel) ? 1 : -1);
		return sortedList.slice(0, count);
	}
	assertCorrectRange(range) {
		if (range.lower !== CUSTOM_MIN_ID || range.upper !== CUSTOM_MAX_ID) throw new ProgrammingError(`Invalid range for CalendarEvent: ${JSON.stringify(range)}`);
	}
	async getElementIdsInCacheRange(storage, listId, ids) {
		const range = await storage.getRangeForList(CalendarEventTypeRef, listId);
		if (range) {
			this.assertCorrectRange(range);
			return ids;
		} else return [];
	}
};
var CustomMailEventCacheHandler = class {
	async shouldLoadOnCreateEvent() {
		return true;
	}
};

//#endregion
//#region ../src/common/api/worker/offline/OfflineStorage.ts
/**
* this is the value of SQLITE_MAX_VARIABLE_NUMBER in sqlite3.c
* it may change if the sqlite version is updated.
* */
const MAX_SAFE_SQL_VARS = 32766;
function dateEncoder(data, typ, options) {
	const time = data.getTime();
	return [new Token(Type$1.tag, 100), new Token(time < 0 ? Type$1.negint : Type$1.uint, time)];
}
function dateDecoder(bytes) {
	return new Date(bytes);
}
const customTypeEncoders = Object.freeze({ Date: dateEncoder });
const customTypeDecoders = (() => {
	const tags = [];
	tags[100] = dateDecoder;
	return tags;
})();
const TableDefinitions = Object.freeze({
	list_entities: "type TEXT NOT NULL, listId TEXT NOT NULL, elementId TEXT NOT NULL, ownerGroup TEXT, entity BLOB NOT NULL, PRIMARY KEY (type, listId, elementId)",
	element_entities: "type TEXT NOT NULL, elementId TEXT NOT NULL, ownerGroup TEXT, entity BLOB NOT NULL, PRIMARY KEY (type, elementId)",
	ranges: "type TEXT NOT NULL, listId TEXT NOT NULL, lower TEXT NOT NULL, upper TEXT NOT NULL, PRIMARY KEY (type, listId)",
	lastUpdateBatchIdPerGroupId: "groupId TEXT NOT NULL, batchId TEXT NOT NULL, PRIMARY KEY (groupId)",
	metadata: "key TEXT NOT NULL, value BLOB, PRIMARY KEY (key)",
	blob_element_entities: "type TEXT NOT NULL, listId TEXT NOT NULL, elementId TEXT NOT NULL, ownerGroup TEXT, entity BLOB NOT NULL, PRIMARY KEY (type, listId, elementId)"
});
var OfflineStorage = class {
	customCacheHandler = null;
	userId = null;
	timeRangeDays = null;
	constructor(sqlCipherFacade, interWindowEventSender, dateProvider, migrator, cleaner) {
		this.sqlCipherFacade = sqlCipherFacade;
		this.interWindowEventSender = interWindowEventSender;
		this.dateProvider = dateProvider;
		this.migrator = migrator;
		this.cleaner = cleaner;
		assert(isOfflineStorageAvailable() || isTest(), "Offline storage is not available.");
	}
	/**
	* @return {boolean} whether the database was newly created or not
	*/
	async init({ userId, databaseKey, timeRangeDays, forceNewDatabase }) {
		this.userId = userId;
		this.timeRangeDays = timeRangeDays;
		if (forceNewDatabase) {
			if (isDesktop()) await this.interWindowEventSender.localUserDataInvalidated(userId);
			await this.sqlCipherFacade.deleteDb(userId);
		}
		await this.sqlCipherFacade.openDb(userId, databaseKey);
		await this.createTables();
		try {
			await this.migrator.migrate(this, this.sqlCipherFacade);
		} catch (e) {
			if (e instanceof OutOfSyncError) {
				console.warn("Offline db is out of sync!", e);
				await this.recreateDbFile(userId, databaseKey);
				await this.migrator.migrate(this, this.sqlCipherFacade);
			} else throw e;
		}
		return (await this.getLastUpdateTime()).type === "never";
	}
	async recreateDbFile(userId, databaseKey) {
		console.log(`recreating DB file for userId ${userId}`);
		await this.sqlCipherFacade.closeDb();
		await this.sqlCipherFacade.deleteDb(userId);
		await this.sqlCipherFacade.openDb(userId, databaseKey);
		await this.createTables();
	}
	/**
	* currently, we close DBs from the native side (mainly on things like reload and on android's onDestroy)
	*/
	async deinit() {
		this.userId = null;
		await this.sqlCipherFacade.closeDb();
	}
	async deleteIfExists(typeRef, listId, elementId) {
		const type = getTypeId(typeRef);
		let typeModel;
		typeModel = await resolveTypeReference(typeRef);
		elementId = ensureBase64Ext(typeModel, elementId);
		let formattedQuery;
		switch (typeModel.type) {
			case Type.Element:
				formattedQuery = sql`DELETE
									 FROM element_entities
									 WHERE type = ${type}
									   AND elementId = ${elementId}`;
				break;
			case Type.ListElement:
				formattedQuery = sql`DELETE
									 FROM list_entities
									 WHERE type = ${type}
									   AND listId = ${listId}
									   AND elementId = ${elementId}`;
				break;
			case Type.BlobElement:
				formattedQuery = sql`DELETE
									 FROM blob_element_entities
									 WHERE type = ${type}
									   AND listId = ${listId}
									   AND elementId = ${elementId}`;
				break;
			default: throw new Error("must be a persistent type");
		}
		await this.sqlCipherFacade.run(formattedQuery.query, formattedQuery.params);
	}
	async deleteAllOfType(typeRef) {
		const type = getTypeId(typeRef);
		let typeModel;
		typeModel = await resolveTypeReference(typeRef);
		let formattedQuery;
		switch (typeModel.type) {
			case Type.Element:
				formattedQuery = sql`DELETE
									 FROM element_entities
									 WHERE type = ${type}`;
				break;
			case Type.ListElement:
				formattedQuery = sql`DELETE
									 FROM list_entities
									 WHERE type = ${type}`;
				await this.sqlCipherFacade.run(formattedQuery.query, formattedQuery.params);
				await this.deleteAllRangesForType(type);
				return;
			case Type.BlobElement:
				formattedQuery = sql`DELETE
									 FROM blob_element_entities
									 WHERE type = ${type}`;
				break;
			default: throw new Error("must be a persistent type");
		}
		await this.sqlCipherFacade.run(formattedQuery.query, formattedQuery.params);
	}
	async deleteAllRangesForType(type) {
		const { query, params } = sql`DELETE
									  FROM ranges
									  WHERE type = ${type}`;
		await this.sqlCipherFacade.run(query, params);
	}
	async get(typeRef, listId, elementId) {
		const type = getTypeId(typeRef);
		const typeModel = await resolveTypeReference(typeRef);
		elementId = ensureBase64Ext(typeModel, elementId);
		let formattedQuery;
		switch (typeModel.type) {
			case Type.Element:
				formattedQuery = sql`SELECT entity
									 from element_entities
									 WHERE type = ${type}
									   AND elementId = ${elementId}`;
				break;
			case Type.ListElement:
				formattedQuery = sql`SELECT entity
									 from list_entities
									 WHERE type = ${type}
									   AND listId = ${listId}
									   AND elementId = ${elementId}`;
				break;
			case Type.BlobElement:
				formattedQuery = sql`SELECT entity
									 from blob_element_entities
									 WHERE type = ${type}
									   AND listId = ${listId}
									   AND elementId = ${elementId}`;
				break;
			default: throw new Error("must be a persistent type");
		}
		const result = await this.sqlCipherFacade.get(formattedQuery.query, formattedQuery.params);
		return result?.entity ? await this.deserialize(typeRef, result.entity.value) : null;
	}
	async provideMultiple(typeRef, listId, elementIds) {
		if (elementIds.length === 0) return [];
		const typeModel = await resolveTypeReference(typeRef);
		elementIds = elementIds.map((el) => ensureBase64Ext(typeModel, el));
		const type = getTypeId(typeRef);
		const serializedList = await this.allChunked(MAX_SAFE_SQL_VARS - 2, elementIds, (c) => sql`SELECT entity
					   FROM list_entities
					   WHERE type = ${type}
						 AND listId = ${listId}
						 AND elementId IN ${paramList(c)}`);
		return await this.deserializeList(typeRef, serializedList.map((r) => r.entity.value));
	}
	async getIdsInRange(typeRef, listId) {
		const type = getTypeId(typeRef);
		const typeModel = await resolveTypeReference(typeRef);
		const range = await this.getRange(typeRef, listId);
		if (range == null) throw new Error(`no range exists for ${type} and list ${listId}`);
		const { query, params } = sql`SELECT elementId
									  FROM list_entities
									  WHERE type = ${type}
										AND listId = ${listId}
										AND (elementId = ${range.lower}
										  OR ${firstIdBigger("elementId", range.lower)})
										AND NOT (${firstIdBigger("elementId", range.upper)})`;
		const rows = await this.sqlCipherFacade.all(query, params);
		return rows.map((row) => customIdToBase64Url(typeModel, row.elementId.value));
	}
	/** don't use this internally in this class, use OfflineStorage::getRange instead. OfflineStorage is
	* using converted custom IDs internally which is undone when using this to access the range.
	*/
	async getRangeForList(typeRef, listId) {
		let range = await this.getRange(typeRef, listId);
		if (range == null) return range;
		const typeModel = await resolveTypeReference(typeRef);
		return {
			lower: customIdToBase64Url(typeModel, range.lower),
			upper: customIdToBase64Url(typeModel, range.upper)
		};
	}
	async isElementIdInCacheRange(typeRef, listId, elementId) {
		const typeModel = await resolveTypeReference(typeRef);
		elementId = ensureBase64Ext(typeModel, elementId);
		const range = await this.getRange(typeRef, listId);
		return range != null && !firstBiggerThanSecond(elementId, range.upper) && !firstBiggerThanSecond(range.lower, elementId);
	}
	async provideFromRange(typeRef, listId, start, count, reverse) {
		const typeModel = await resolveTypeReference(typeRef);
		start = ensureBase64Ext(typeModel, start);
		const type = getTypeId(typeRef);
		let formattedQuery;
		if (reverse) formattedQuery = sql`SELECT entity
								 FROM list_entities
								 WHERE type = ${type}
								   AND listId = ${listId}
								   AND ${firstIdBigger(start, "elementId")}
								 ORDER BY LENGTH(elementId) DESC, elementId DESC LIMIT ${count}`;
else formattedQuery = sql`SELECT entity
								 FROM list_entities
								 WHERE type = ${type}
								   AND listId = ${listId}
								   AND ${firstIdBigger("elementId", start)}
								 ORDER BY LENGTH(elementId) ASC, elementId ASC LIMIT ${count}`;
		const { query, params } = formattedQuery;
		const serializedList = await this.sqlCipherFacade.all(query, params);
		return await this.deserializeList(typeRef, serializedList.map((r) => r.entity.value));
	}
	async put(originalEntity) {
		const serializedEntity = this.serialize(originalEntity);
		let { listId, elementId } = expandId(originalEntity._id);
		const type = getTypeId(originalEntity._type);
		const ownerGroup = originalEntity._ownerGroup;
		const typeModel = await resolveTypeReference(originalEntity._type);
		elementId = ensureBase64Ext(typeModel, elementId);
		let formattedQuery;
		switch (typeModel.type) {
			case Type.Element:
				formattedQuery = sql`INSERT
				OR REPLACE INTO element_entities (type, elementId, ownerGroup, entity) VALUES (
				${type},
				${elementId},
				${ownerGroup},
				${serializedEntity}
				)`;
				break;
			case Type.ListElement:
				formattedQuery = sql`INSERT
				OR REPLACE INTO list_entities (type, listId, elementId, ownerGroup, entity) VALUES (
				${type},
				${listId},
				${elementId},
				${ownerGroup},
				${serializedEntity}
				)`;
				break;
			case Type.BlobElement:
				formattedQuery = sql`INSERT
				OR REPLACE INTO blob_element_entities (type, listId, elementId, ownerGroup, entity) VALUES (
				${type},
				${listId},
				${elementId},
				${ownerGroup},
				${serializedEntity}
				)`;
				break;
			default: throw new Error("must be a persistent type");
		}
		await this.sqlCipherFacade.run(formattedQuery.query, formattedQuery.params);
	}
	async setLowerRangeForList(typeRef, listId, lowerId) {
		lowerId = ensureBase64Ext(await resolveTypeReference(typeRef), lowerId);
		const type = getTypeId(typeRef);
		const { query, params } = sql`UPDATE ranges
									  SET lower = ${lowerId}
									  WHERE type = ${type}
										AND listId = ${listId}`;
		await this.sqlCipherFacade.run(query, params);
	}
	async setUpperRangeForList(typeRef, listId, upperId) {
		upperId = ensureBase64Ext(await resolveTypeReference(typeRef), upperId);
		const type = getTypeId(typeRef);
		const { query, params } = sql`UPDATE ranges
									  SET upper = ${upperId}
									  WHERE type = ${type}
										AND listId = ${listId}`;
		await this.sqlCipherFacade.run(query, params);
	}
	async setNewRangeForList(typeRef, listId, lower, upper) {
		const typeModel = await resolveTypeReference(typeRef);
		lower = ensureBase64Ext(typeModel, lower);
		upper = ensureBase64Ext(typeModel, upper);
		const type = getTypeId(typeRef);
		const { query, params } = sql`INSERT
		OR REPLACE INTO ranges VALUES (
		${type},
		${listId},
		${lower},
		${upper}
		)`;
		return this.sqlCipherFacade.run(query, params);
	}
	async getLastBatchIdForGroup(groupId) {
		const { query, params } = sql`SELECT batchId
									  from lastUpdateBatchIdPerGroupId
									  WHERE groupId = ${groupId}`;
		const row = await this.sqlCipherFacade.get(query, params);
		return row?.batchId?.value ?? null;
	}
	async putLastBatchIdForGroup(groupId, batchId) {
		const { query, params } = sql`INSERT
		OR REPLACE INTO lastUpdateBatchIdPerGroupId VALUES (
		${groupId},
		${batchId}
		)`;
		await this.sqlCipherFacade.run(query, params);
	}
	async getLastUpdateTime() {
		const time = await this.getMetadata("lastUpdateTime");
		return time ? {
			type: "recorded",
			time
		} : { type: "never" };
	}
	async putLastUpdateTime(ms) {
		await this.putMetadata("lastUpdateTime", ms);
	}
	async purgeStorage() {
		for (let name of Object.keys(TableDefinitions)) await this.sqlCipherFacade.run(`DELETE
				 FROM ${name}`, []);
	}
	async deleteRange(typeRef, listId) {
		const { query, params } = sql`DELETE
									  FROM ranges
									  WHERE type = ${getTypeId(typeRef)}
										AND listId = ${listId}`;
		await this.sqlCipherFacade.run(query, params);
	}
	async getRawListElementsOfType(typeRef) {
		const { query, params } = sql`SELECT entity
									  from list_entities
									  WHERE type = ${getTypeId(typeRef)}`;
		const items = await this.sqlCipherFacade.all(query, params) ?? [];
		return items.map((item) => this.decodeCborEntity(item.entity.value));
	}
	async getRawElementsOfType(typeRef) {
		const { query, params } = sql`SELECT entity
									  from element_entities
									  WHERE type = ${getTypeId(typeRef)}`;
		const items = await this.sqlCipherFacade.all(query, params) ?? [];
		return items.map((item) => this.decodeCborEntity(item.entity.value));
	}
	async getElementsOfType(typeRef) {
		const { query, params } = sql`SELECT entity
									  from element_entities
									  WHERE type = ${getTypeId(typeRef)}`;
		const items = await this.sqlCipherFacade.all(query, params) ?? [];
		return await this.deserializeList(typeRef, items.map((row) => row.entity.value));
	}
	async getWholeList(typeRef, listId) {
		const { query, params } = sql`SELECT entity
									  FROM list_entities
									  WHERE type = ${getTypeId(typeRef)}
										AND listId = ${listId}`;
		const items = await this.sqlCipherFacade.all(query, params) ?? [];
		return await this.deserializeList(typeRef, items.map((row) => row.entity.value));
	}
	async dumpMetadata() {
		const query = "SELECT * from metadata";
		const stored = (await this.sqlCipherFacade.all(query, [])).map((row) => [row.key.value, row.value.value]);
		return Object.fromEntries(stored.map(([key, value]) => [key, decode(value)]));
	}
	async setStoredModelVersion(model, version) {
		return this.putMetadata(`${model}-version`, version);
	}
	getCustomCacheHandlerMap(entityRestClient) {
		if (this.customCacheHandler == null) this.customCacheHandler = new CustomCacheHandlerMap({
			ref: CalendarEventTypeRef,
			handler: new CustomCalendarEventCacheHandler(entityRestClient)
		}, {
			ref: MailTypeRef,
			handler: new CustomMailEventCacheHandler()
		});
		return this.customCacheHandler;
	}
	getUserId() {
		return assertNotNull(this.userId, "No user id, not initialized?");
	}
	async deleteAllOwnedBy(owner) {
		{
			const { query, params } = sql`DELETE
										  FROM element_entities
										  WHERE ownerGroup = ${owner}`;
			await this.sqlCipherFacade.run(query, params);
		}
		{
			const { query, params } = sql`SELECT listId, type
										  FROM list_entities
										  WHERE ownerGroup = ${owner}`;
			const rangeRows = await this.sqlCipherFacade.all(query, params);
			const rows = rangeRows.map((row) => untagSqlObject(row));
			const listIdsByType = groupByAndMapUniquely(rows, (row) => row.type, (row) => row.listId);
			for (const [type, listIds] of listIdsByType.entries()) {
				const safeChunkSize = MAX_SAFE_SQL_VARS - 1;
				const listIdArr = Array.from(listIds);
				await this.runChunked(safeChunkSize, listIdArr, (c) => sql`DELETE
							   FROM ranges
							   WHERE type = ${type}
								 AND listId IN ${paramList(c)}`);
				await this.runChunked(safeChunkSize, listIdArr, (c) => sql`DELETE
							   FROM list_entities
							   WHERE type = ${type}
								 AND listId IN ${paramList(c)}`);
			}
		}
		{
			const { query, params } = sql`DELETE
										  FROM blob_element_entities
										  WHERE ownerGroup = ${owner}`;
			await this.sqlCipherFacade.run(query, params);
		}
		{
			const { query, params } = sql`DELETE
										  FROM lastUpdateBatchIdPerGroupId
										  WHERE groupId = ${owner}`;
			await this.sqlCipherFacade.run(query, params);
		}
	}
	async deleteWholeList(typeRef, listId) {
		await this.lockRangesDbAccess(listId);
		await this.deleteRange(typeRef, listId);
		const { query, params } = sql`DELETE
									  FROM list_entities
									  WHERE listId = ${listId}`;
		await this.sqlCipherFacade.run(query, params);
		await this.unlockRangesDbAccess(listId);
	}
	async putMetadata(key, value) {
		let encodedValue;
		try {
			encodedValue = encode(value);
		} catch (e) {
			console.log("[OfflineStorage] failed to encode metadata for key", key, "with value", value);
			throw e;
		}
		const { query, params } = sql`INSERT
		OR REPLACE INTO metadata VALUES (
		${key},
		${encodedValue}
		)`;
		await this.sqlCipherFacade.run(query, params);
	}
	async getMetadata(key) {
		const { query, params } = sql`SELECT value
									  from metadata
									  WHERE key = ${key}`;
		const encoded = await this.sqlCipherFacade.get(query, params);
		return encoded && decode(encoded.value.value);
	}
	/**
	* Clear out unneeded data from the offline database (i.e. trash and spam lists, old data).
	* This will be called after login (CachePostLoginActions.ts) to ensure fast login time.
	* @param timeRangeDays: the maximum age of days that mails should be to be kept in the database. if null, will use a default value
	* @param userId id of the current user. default, last stored userId
	*/
	async clearExcludedData(timeRangeDays = this.timeRangeDays, userId = this.getUserId()) {
		await this.cleaner.cleanOfflineDb(this, timeRangeDays, userId, this.dateProvider.now());
	}
	async createTables() {
		for (let [name, definition] of Object.entries(TableDefinitions)) await this.sqlCipherFacade.run(`CREATE TABLE IF NOT EXISTS ${name}
				 (
					 ${definition}
				 )`, []);
	}
	async getRange(typeRef, listId) {
		const type = getTypeId(typeRef);
		const { query, params } = sql`SELECT upper, lower
									  FROM ranges
									  WHERE type = ${type}
										AND listId = ${listId}`;
		const row = await this.sqlCipherFacade.get(query, params) ?? null;
		return mapNullable(row, untagSqlObject);
	}
	async deleteIn(typeRef, listId, elementIds) {
		if (elementIds.length === 0) return;
		const typeModel = await resolveTypeReference(typeRef);
		switch (typeModel.type) {
			case Type.Element: return await this.runChunked(MAX_SAFE_SQL_VARS - 1, elementIds, (c) => sql`DELETE
							   FROM element_entities
							   WHERE type = ${getTypeId(typeRef)}
								 AND elementId IN ${paramList(c)}`);
			case Type.ListElement: return await this.runChunked(MAX_SAFE_SQL_VARS - 2, elementIds, (c) => sql`DELETE
							   FROM list_entities
							   WHERE type = ${getTypeId(typeRef)}
								 AND listId = ${listId}
								 AND elementId IN ${paramList(c)}`);
			case Type.BlobElement: return await this.runChunked(MAX_SAFE_SQL_VARS - 2, elementIds, (c) => sql`DELETE
							   FROM blob_element_entities
							   WHERE type = ${getTypeId(typeRef)}
								 AND listId = ${listId}
								 AND elementId IN ${paramList(c)}`);
			default: throw new Error("must be a persistent type");
		}
	}
	/**
	* We want to lock the access to the "ranges" db when updating / reading the
	* offline available mail list / mailset ranges for each mail list (referenced using the listId).
	* @param listId the mail list or mail set entry list that we want to lock
	*/
	async lockRangesDbAccess(listId) {
		await this.sqlCipherFacade.lockRangesDbAccess(listId);
	}
	/**
	* This is the counterpart to the function "lockRangesDbAccess(listId)".
	* @param listId the mail list that we want to unlock
	*/
	async unlockRangesDbAccess(listId) {
		await this.sqlCipherFacade.unlockRangesDbAccess(listId);
	}
	async updateRangeForListAndDeleteObsoleteData(typeRef, listId, rawCutoffId) {
		const typeModel = await resolveTypeReference(typeRef);
		const isCustomId = isCustomIdType(typeModel);
		const convertedCutoffId = ensureBase64Ext(typeModel, rawCutoffId);
		const range = await this.getRange(typeRef, listId);
		if (range == null) return;
		const expectedMinId = isCustomId ? CUSTOM_MIN_ID : GENERATED_MIN_ID;
		if (range.lower === expectedMinId) {
			const entities = await this.provideFromRange(typeRef, listId, expectedMinId, 1, false);
			const id = mapNullable(entities[0], getElementId);
			const rangeWontBeModified = id == null || firstBiggerThanSecond(id, convertedCutoffId) || id === convertedCutoffId;
			if (rangeWontBeModified) return;
		}
		if (firstBiggerThanSecond(convertedCutoffId, range.lower)) if (firstBiggerThanSecond(convertedCutoffId, range.upper)) await this.deleteRange(typeRef, listId);
else await this.setLowerRangeForList(typeRef, listId, rawCutoffId);
	}
	serialize(originalEntity) {
		try {
			return encode(originalEntity, { typeEncoders: customTypeEncoders });
		} catch (e) {
			console.log("[OfflineStorage] failed to encode entity of type", originalEntity._type, "with id", originalEntity._id);
			throw e;
		}
	}
	/**
	* Convert the type from CBOR representation to the runtime type
	*/
	async deserialize(typeRef, loaded) {
		let deserialized;
		try {
			deserialized = this.decodeCborEntity(loaded);
		} catch (e) {
			console.log(e);
			console.log(`Error with CBOR decode. Trying to decode (of type: ${typeof loaded}): ${loaded}`);
			return null;
		}
		const typeModel = await resolveTypeReference(typeRef);
		return await this.fixupTypeRefs(typeModel, deserialized);
	}
	decodeCborEntity(loaded) {
		return decode(loaded, { tags: customTypeDecoders });
	}
	async fixupTypeRefs(typeModel, deserialized) {
		deserialized._type = new TypeRef(typeModel.app, typeModel.name);
		for (const [associationName, associationModel] of Object.entries(typeModel.associations)) if (associationModel.type === AssociationType.Aggregation) {
			const aggregateTypeRef = new TypeRef(associationModel.dependency ?? typeModel.app, associationModel.refType);
			const aggregateTypeModel = await resolveTypeReference(aggregateTypeRef);
			switch (associationModel.cardinality) {
				case Cardinality.One:
				case Cardinality.ZeroOrOne: {
					const aggregate = deserialized[associationName];
					if (aggregate) await this.fixupTypeRefs(aggregateTypeModel, aggregate);
					break;
				}
				case Cardinality.Any: {
					const aggregateList = deserialized[associationName];
					for (const aggregate of aggregateList) await this.fixupTypeRefs(aggregateTypeModel, aggregate);
					break;
				}
			}
		}
		return deserialized;
	}
	async deserializeList(typeRef, loaded) {
		const result = [];
		for (const entity of loaded) {
			const deserialized = await this.deserialize(typeRef, entity);
			if (deserialized != null) result.push(deserialized);
		}
		return result;
	}
	/**
	* convenience method to run a potentially too large query over several chunks.
	* chunkSize must be chosen such that the total number of SQL variables in the final query does not exceed MAX_SAFE_SQL_VARS
	* */
	async runChunked(chunkSize, originalList, formatter) {
		for (const chunk of splitInChunks(chunkSize, originalList)) {
			const formattedQuery = formatter(chunk);
			await this.sqlCipherFacade.run(formattedQuery.query, formattedQuery.params);
		}
	}
	/**
	* convenience method to execute a potentially too large query over several chunks.
	* chunkSize must be chosen such that the total number of SQL variables in the final query does not exceed MAX_SAFE_SQL_VARS
	* */
	async allChunked(chunkSize, originalList, formatter) {
		const result = [];
		for (const chunk of splitInChunks(chunkSize, originalList)) {
			const formattedQuery = formatter(chunk);
			result.push(...await this.sqlCipherFacade.all(formattedQuery.query, formattedQuery.params));
		}
		return result;
	}
};
function paramList(params) {
	const qs = params.map(() => "?").join(",");
	return new SqlFragment(`(${qs})`, params);
}
/**
* comparison to select ids that are bigger or smaller than a parameter id
* must be used within sql`<query>` template string to inline the logic into the query.
*
* will always insert 3 constants and 3 SQL variables into the query.
*/
function firstIdBigger(...args) {
	let [l, r] = args;
	let v;
	if (l === "elementId") {
		v = r;
		r = "?";
	} else {
		v = l;
		l = "?";
	}
	return new SqlFragment(`(CASE WHEN length(${l}) > length(${r}) THEN 1 WHEN length(${l}) < length(${r}) THEN 0 ELSE ${l} > ${r} END)`, [
		v,
		v,
		v
	]);
}
function isCustomIdType(typeModel) {
	return typeModel.values._id.type === ValueType.CustomId;
}
function ensureBase64Ext(typeModel, elementId) {
	if (isCustomIdType(typeModel)) return base64ToBase64Ext(base64UrlToBase64(elementId));
	return elementId;
}
function customIdToBase64Url(typeModel, elementId) {
	if (isCustomIdType(typeModel)) return base64ToBase64Url(base64ExtToBase64(elementId));
	return elementId;
}

//#endregion
//#region ../src/common/api/worker/rest/DefaultEntityRestCache.ts
assertWorkerOrNode();
const EXTEND_RANGE_MIN_CHUNK_SIZE = 40;
const IGNORED_TYPES = [
	EntityEventBatchTypeRef,
	PermissionTypeRef,
	BucketPermissionTypeRef,
	SessionTypeRef,
	SecondFactorTypeRef,
	RecoverCodeTypeRef,
	RejectedSenderTypeRef,
	CalendarEventUidIndexTypeRef,
	KeyRotationTypeRef,
	UserGroupRootTypeRef,
	UserGroupKeyDistributionTypeRef,
	AuditLogEntryTypeRef
];
/**
* List of types containing a customId that we want to explicitly enable caching for.
* CustomId types are not cached by default because their id is using base64UrlEncoding while GeneratedUId types are using base64Ext encoding.
* base64Url encoding results in a different sort order of elements that we have on the server, this is problematic for caching LET and their ranges.
* When enabling caching for customId types we convert the id that we store in cache from base64Url to base64Ext so we have the same sort order. (see function
* OfflineStorage.ensureBase64Ext). In theory, we can try to enable caching for all types but as of now we enable it for a limited amount of types because there
* are other ways to cache customId types (see implementation of CustomCacheHandler)
*/
const CACHEABLE_CUSTOMID_TYPES = [MailSetEntryTypeRef, GroupKeyTypeRef];
var DefaultEntityRestCache = class {
	constructor(entityRestClient, storage) {
		this.entityRestClient = entityRestClient;
		this.storage = storage;
	}
	async load(typeRef, id, opts = {}) {
		const useCache = await this.shouldUseCache(typeRef, opts);
		if (!useCache) return await this.entityRestClient.load(typeRef, id, opts);
		const { listId, elementId } = expandId(id);
		const cachingBehavior = getCacheModeBehavior(opts.cacheMode);
		const cachedEntity = cachingBehavior.readsFromCache ? await this.storage.get(typeRef, listId, elementId) : null;
		if (cachedEntity == null) {
			const entity = await this.entityRestClient.load(typeRef, id, opts);
			if (cachingBehavior.writesToCache) await this.storage.put(entity);
			return entity;
		}
		return cachedEntity;
	}
	async loadMultiple(typeRef, listId, ids, ownerEncSessionKeyProvider, opts = {}) {
		const useCache = await this.shouldUseCache(typeRef, opts);
		if (!useCache) return await this.entityRestClient.loadMultiple(typeRef, listId, ids, ownerEncSessionKeyProvider, opts);
		return await this._loadMultiple(typeRef, listId, ids, ownerEncSessionKeyProvider, opts);
	}
	setup(listId, instance, extraHeaders, options) {
		return this.entityRestClient.setup(listId, instance, extraHeaders, options);
	}
	setupMultiple(listId, instances) {
		return this.entityRestClient.setupMultiple(listId, instances);
	}
	update(instance) {
		return this.entityRestClient.update(instance);
	}
	erase(instance, options) {
		return this.entityRestClient.erase(instance, options);
	}
	getLastEntityEventBatchForGroup(groupId) {
		return this.storage.getLastBatchIdForGroup(groupId);
	}
	setLastEntityEventBatchForGroup(groupId, batchId) {
		return this.storage.putLastBatchIdForGroup(groupId, batchId);
	}
	purgeStorage() {
		console.log("Purging the user's offline database");
		return this.storage.purgeStorage();
	}
	async isOutOfSync() {
		const timeSinceLastSync = await this.timeSinceLastSyncMs();
		return timeSinceLastSync != null && timeSinceLastSync > ENTITY_EVENT_BATCH_EXPIRE_MS;
	}
	async recordSyncTime() {
		const timestamp = this.getServerTimestampMs();
		await this.storage.putLastUpdateTime(timestamp);
	}
	async timeSinceLastSyncMs() {
		const lastUpdate = await this.storage.getLastUpdateTime();
		let lastUpdateTime;
		switch (lastUpdate.type) {
			case "recorded":
				lastUpdateTime = lastUpdate.time;
				break;
			case "never": return null;
			case "uninitialized": throw new ProgrammingError("Offline storage is not initialized");
		}
		const now = this.getServerTimestampMs();
		return now - lastUpdateTime;
	}
	getServerTimestampMs() {
		return this.entityRestClient.getRestClient().getServerTimestampMs();
	}
	/**
	* Delete a cached entity. Sometimes this is necessary to do to ensure you always load the new version
	*/
	deleteFromCacheIfExists(typeRef, listId, elementId) {
		return this.storage.deleteIfExists(typeRef, listId, elementId);
	}
	async _loadMultiple(typeRef, listId, ids, ownerEncSessionKeyProvider, opts = {}) {
		const cachingBehavior = getCacheModeBehavior(opts.cacheMode);
		const entitiesInCache = [];
		let idsToLoad;
		if (cachingBehavior.readsFromCache) {
			idsToLoad = [];
			for (const id of ids) {
				const cachedEntity = await this.storage.get(typeRef, listId, id);
				if (cachedEntity != null) entitiesInCache.push(cachedEntity);
else idsToLoad.push(id);
			}
		} else idsToLoad = ids;
		if (idsToLoad.length > 0) {
			const entitiesFromServer = await this.entityRestClient.loadMultiple(typeRef, listId, idsToLoad, ownerEncSessionKeyProvider, opts);
			if (cachingBehavior.writesToCache) for (const entity of entitiesFromServer) await this.storage.put(entity);
			return entitiesFromServer.concat(entitiesInCache);
		} else return entitiesInCache;
	}
	async loadRange(typeRef, listId, start, count, reverse, opts = {}) {
		const customHandler = this.storage.getCustomCacheHandlerMap(this.entityRestClient).get(typeRef);
		if (customHandler && customHandler.loadRange) return await customHandler.loadRange(this.storage, listId, start, count, reverse);
		const typeModel = await resolveTypeReference(typeRef);
		const useCache = await this.shouldUseCache(typeRef, opts) && isCachedRangeType(typeModel, typeRef);
		if (!useCache) return await this.entityRestClient.loadRange(typeRef, listId, start, count, reverse, opts);
		const behavior = getCacheModeBehavior(opts.cacheMode);
		if (!behavior.readsFromCache) throw new ProgrammingError("cannot write to cache without reading with range requests");
		await this.storage.lockRangesDbAccess(listId);
		try {
			const range = await this.storage.getRangeForList(typeRef, listId);
			if (behavior.writesToCache) {
				if (range == null) await this.populateNewListWithRange(typeRef, listId, start, count, reverse, opts);
else if (isStartIdWithinRange(range, start, typeModel)) await this.extendFromWithinRange(typeRef, listId, start, count, reverse, opts);
else if (isRangeRequestAwayFromExistingRange(range, reverse, start, typeModel)) await this.extendAwayFromRange(typeRef, listId, start, count, reverse, opts);
else await this.extendTowardsRange(typeRef, listId, start, count, reverse, opts);
				return await this.storage.provideFromRange(typeRef, listId, start, count, reverse);
			} else if (range && isStartIdWithinRange(range, start, typeModel)) {
				const provided = await this.storage.provideFromRange(typeRef, listId, start, count, reverse);
				const { newStart, newCount } = await this.recalculateRangeRequest(typeRef, listId, start, count, reverse);
				const newElements = newCount > 0 ? await this.entityRestClient.loadRange(typeRef, listId, newStart, newCount, reverse) : [];
				return provided.concat(newElements);
			} else return await this.entityRestClient.loadRange(typeRef, listId, start, count, reverse, opts);
		} finally {
			await this.storage.unlockRangesDbAccess(listId);
		}
	}
	/**
	* Creates a new list range, reading everything from the server that it can
	* range:         (none)
	* request:       *--------->
	* range becomes: |---------|
	* @private
	*/
	async populateNewListWithRange(typeRef, listId, start, count, reverse, opts) {
		const entities = await this.entityRestClient.loadRange(typeRef, listId, start, count, reverse, opts);
		await this.storage.setNewRangeForList(typeRef, listId, start, start);
		await this.updateRangeInStorage(typeRef, listId, count, reverse, entities);
	}
	/**
	* Returns part of a request from the cache, and the remainder is loaded from the server
	* range:          |---------|
	* request:             *-------------->
	* range becomes: |--------------------|
	*/
	async extendFromWithinRange(typeRef, listId, start, count, reverse, opts) {
		const { newStart, newCount } = await this.recalculateRangeRequest(typeRef, listId, start, count, reverse);
		if (newCount > 0) {
			const entities = await this.entityRestClient.loadRange(typeRef, listId, newStart, newCount, reverse, opts);
			await this.updateRangeInStorage(typeRef, listId, newCount, reverse, entities);
		}
	}
	/**
	* Start was outside the range, and we are loading away from the range
	* Keeps loading elements from the end of the range in the direction of the startId.
	* Returns once all available elements have been loaded or the requested number is in cache
	* range:          |---------|
	* request:                     *------->
	* range becomes:  |--------------------|
	*/
	async extendAwayFromRange(typeRef, listId, start, count, reverse, opts) {
		while (true) {
			const range = assertNotNull(await this.storage.getRangeForList(typeRef, listId));
			const loadStartId = reverse ? range.lower : range.upper;
			const requestCount = Math.max(count, EXTEND_RANGE_MIN_CHUNK_SIZE);
			const entities = await this.entityRestClient.loadRange(typeRef, listId, loadStartId, requestCount, reverse, opts);
			await this.updateRangeInStorage(typeRef, listId, requestCount, reverse, entities);
			if (entities.length < requestCount) break;
			const entitiesFromCache = await this.storage.provideFromRange(typeRef, listId, start, count, reverse);
			if (entitiesFromCache.length === count) break;
		}
	}
	/**
	* Loads all elements from the startId in the direction of the range
	* Once complete, returns as many elements as it can from the original request
	* range:         |---------|
	* request:                     <------*
	* range becomes: |--------------------|
	* or
	* range:              |---------|
	* request:       <-------------------*
	* range becomes: |--------------------|
	*/
	async extendTowardsRange(typeRef, listId, start, count, reverse, opts) {
		while (true) {
			const range = assertNotNull(await this.storage.getRangeForList(typeRef, listId));
			const loadStartId = reverse ? range.upper : range.lower;
			const requestCount = Math.max(count, EXTEND_RANGE_MIN_CHUNK_SIZE);
			const entities = await this.entityRestClient.loadRange(typeRef, listId, loadStartId, requestCount, !reverse, opts);
			await this.updateRangeInStorage(typeRef, listId, requestCount, !reverse, entities);
			if (await this.storage.isElementIdInCacheRange(typeRef, listId, start)) break;
		}
		await this.extendFromWithinRange(typeRef, listId, start, count, reverse, opts);
	}
	/**
	* Given the parameters and result of a range request,
	* Inserts the result into storage, and updates the range bounds
	* based on number of entities requested and the actual amount that were received
	*/
	async updateRangeInStorage(typeRef, listId, countRequested, wasReverseRequest, receivedEntities) {
		const isCustomId = isCustomIdType(await resolveTypeReference(typeRef));
		let elementsToAdd = receivedEntities;
		if (wasReverseRequest) {
			elementsToAdd = receivedEntities.reverse();
			if (receivedEntities.length < countRequested) {
				console.log("finished loading, setting min id");
				await this.storage.setLowerRangeForList(typeRef, listId, isCustomId ? CUSTOM_MIN_ID : GENERATED_MIN_ID);
			} else await this.storage.setLowerRangeForList(typeRef, listId, getElementId(getFirstOrThrow(receivedEntities)));
		} else if (receivedEntities.length < countRequested) {
			console.log("finished loading, setting max id");
			await this.storage.setUpperRangeForList(typeRef, listId, isCustomId ? CUSTOM_MAX_ID : GENERATED_MAX_ID);
		} else await this.storage.setUpperRangeForList(typeRef, listId, getElementId(lastThrow(receivedEntities)));
		await Promise.all(elementsToAdd.map((element) => this.storage.put(element)));
	}
	/**
	* Calculates the new start value for the getElementRange request and the number of elements to read in
	* order to read no duplicate values.
	* @return returns the new start and count value. Important: count can be negative if everything is cached
	*/
	async recalculateRangeRequest(typeRef, listId, start, count, reverse) {
		let allRangeList = await this.storage.getIdsInRange(typeRef, listId);
		let elementsToRead = count;
		let startElementId = start;
		const range = await this.storage.getRangeForList(typeRef, listId);
		if (range == null) return {
			newStart: start,
			newCount: count
		};
		const { lower, upper } = range;
		let indexOfStart = allRangeList.indexOf(start);
		const typeModel = await resolveTypeReference(typeRef);
		const isCustomId = isCustomIdType(typeModel);
		if (!reverse && (isCustomId ? upper === CUSTOM_MAX_ID : upper === GENERATED_MAX_ID) || reverse && (isCustomId ? lower === CUSTOM_MIN_ID : lower === GENERATED_MIN_ID)) elementsToRead = 0;
else if (allRangeList.length === 0) elementsToRead = count;
else if (indexOfStart !== -1) if (reverse) {
			elementsToRead = count - indexOfStart;
			startElementId = allRangeList[0];
		} else {
			elementsToRead = count - (allRangeList.length - 1 - indexOfStart);
			startElementId = allRangeList[allRangeList.length - 1];
		}
else if (lower === start || firstBiggerThanSecond(start, lower, typeModel) && firstBiggerThanSecond(allRangeList[0], start, typeModel)) {
			if (!reverse) {
				startElementId = allRangeList[allRangeList.length - 1];
				elementsToRead = count - allRangeList.length;
			}
		} else if (upper === start || firstBiggerThanSecond(start, allRangeList[allRangeList.length - 1], typeModel) && firstBiggerThanSecond(upper, start, typeModel)) {
			if (reverse) {
				startElementId = allRangeList[0];
				elementsToRead = count - allRangeList.length;
			}
		}
		return {
			newStart: startElementId,
			newCount: elementsToRead
		};
	}
	/**
	* Resolves when the entity is loaded from the server if necessary
	* @pre The last call of this function must be resolved. This is needed to avoid that e.g. while
	* loading a created instance from the server we receive an update of that instance and ignore it because the instance is not in the cache yet.
	*
	* @return Promise, which resolves to the array of valid events (if response is NotFound or NotAuthorized we filter it out)
	*/
	async entityEventsReceived(batch) {
		await this.recordSyncTime();
		const createUpdatesForLETs = [];
		const regularUpdates = [];
		const updatesArray = batch.events;
		for (const update of updatesArray) {
			const typeRef = new TypeRef(update.application, update.type);
			if (update.application === "monitor") continue;
			if (update.operation === OperationType.CREATE && getUpdateInstanceId(update).instanceListId != null && !isSameTypeRef(typeRef, MailTypeRef)) createUpdatesForLETs.push(update);
else {
				regularUpdates.push(update);
				await this.checkForMailSetMigration(typeRef, update);
			}
		}
		const createUpdatesForLETsPerList = groupBy(createUpdatesForLETs, (update) => update.instanceListId);
		const postMultipleEventUpdates = [];
		for (let [instanceListId, updates] of createUpdatesForLETsPerList) {
			const firstUpdate = updates[0];
			const typeRef = new TypeRef(firstUpdate.application, firstUpdate.type);
			const ids = updates.map((update) => update.instanceId);
			const customHandler = this.storage.getCustomCacheHandlerMap(this.entityRestClient).get(typeRef);
			const idsInCacheRange = customHandler && customHandler.getElementIdsInCacheRange ? await customHandler.getElementIdsInCacheRange(this.storage, instanceListId, ids) : await this.getElementIdsInCacheRange(typeRef, instanceListId, ids);
			if (idsInCacheRange.length === 0) postMultipleEventUpdates.push(updates);
else {
				const updatesNotInCacheRange = idsInCacheRange.length === updates.length ? [] : updates.filter((update) => !idsInCacheRange.includes(update.instanceId));
				try {
					const returnedInstances = await this._loadMultiple(typeRef, instanceListId, idsInCacheRange, undefined, { cacheMode: CacheMode.WriteOnly });
					if (returnedInstances.length !== idsInCacheRange.length) {
						const returnedIds = returnedInstances.map((instance) => getElementId(instance));
						postMultipleEventUpdates.push(updates.filter((update) => returnedIds.includes(update.instanceId)).concat(updatesNotInCacheRange));
					} else postMultipleEventUpdates.push(updates);
				} catch (e) {
					if (e instanceof NotAuthorizedError) postMultipleEventUpdates.push(updatesNotInCacheRange);
else throw e;
				}
			}
		}
		const otherEventUpdates = [];
		for (let update of regularUpdates) {
			const { operation, type, application } = update;
			const { instanceListId, instanceId } = getUpdateInstanceId(update);
			const typeRef = new TypeRef(application, type);
			switch (operation) {
				case OperationType.UPDATE: {
					const handledUpdate = await this.processUpdateEvent(typeRef, update);
					if (handledUpdate) otherEventUpdates.push(handledUpdate);
					break;
				}
				case OperationType.DELETE: {
					if (isSameTypeRef(MailTypeRef, typeRef) && containsEventOfType(updatesArray, OperationType.CREATE, instanceId)) {} else if (isSameTypeRef(MailTypeRef, typeRef)) {
						const mail = await this.storage.get(MailTypeRef, instanceListId, instanceId);
						await this.storage.deleteIfExists(typeRef, instanceListId, instanceId);
						if (mail?.mailDetails != null) await this.storage.deleteIfExists(MailDetailsBlobTypeRef, mail.mailDetails[0], mail.mailDetails[1]);
					} else await this.storage.deleteIfExists(typeRef, instanceListId, instanceId);
					otherEventUpdates.push(update);
					break;
				}
				case OperationType.CREATE: {
					const handledUpdate = await this.processCreateEvent(typeRef, update, updatesArray);
					if (handledUpdate) otherEventUpdates.push(handledUpdate);
					break;
				}
				default: throw new ProgrammingError("Unknown operation type: " + operation);
			}
		}
		await this.storage.putLastBatchIdForGroup(batch.groupId, batch.batchId);
		return otherEventUpdates.concat(postMultipleEventUpdates.flat());
	}
	/** Returns {null} when the update should be skipped. */
	async processCreateEvent(typeRef, update, batch) {
		const { instanceId, instanceListId } = getUpdateInstanceId(update);
		if (instanceListId != null) {
			const deleteEvent = getEventOfType(batch, OperationType.DELETE, instanceId);
			const mail = deleteEvent && isSameTypeRef(MailTypeRef, typeRef) ? await this.storage.get(MailTypeRef, deleteEvent.instanceListId, instanceId) : null;
			if (deleteEvent != null && mail != null && isEmpty(mail.sets)) {
				await this.storage.deleteIfExists(typeRef, deleteEvent.instanceListId, instanceId);
				await this.updateListIdOfMailAndUpdateCache(mail, instanceListId, instanceId);
				return update;
			} else {
				const shouldLoad = await this.storage.getCustomCacheHandlerMap(this.entityRestClient).get(typeRef)?.shouldLoadOnCreateEvent?.(update) ?? await this.storage.isElementIdInCacheRange(typeRef, instanceListId, instanceId);
				if (shouldLoad) {
					console.log("downloading create event for", getTypeId(typeRef), instanceListId, instanceId);
					return this.entityRestClient.load(typeRef, [instanceListId, instanceId]).then((entity) => this.storage.put(entity)).then(() => update).catch((e) => {
						if (isExpectedErrorForSynchronization(e)) return null;
else throw e;
					});
				} else return update;
			}
		} else return update;
	}
	/**
	* Updates the given mail with the new list id and add it to the cache.
	*/
	async updateListIdOfMailAndUpdateCache(mail, newListId, elementId) {
		mail._id = [newListId, elementId];
		if (mail.bucketKey != null) {
			const mailSessionKey = mail.bucketKey.bucketEncSessionKeys.find((bucketEncSessionKey) => isSameId(bucketEncSessionKey.instanceId, elementId));
			if (mailSessionKey) mailSessionKey.instanceList = newListId;
		}
		await this.storage.put(mail);
	}
	/** Returns {null} when the update should be skipped. */
	async processUpdateEvent(typeRef, update) {
		const { instanceId, instanceListId } = getUpdateInstanceId(update);
		const cached = await this.storage.get(typeRef, instanceListId, instanceId);
		if (cached != null) try {
			const newEntity = await this.entityRestClient.load(typeRef, collapseId(instanceListId, instanceId));
			if (isSameTypeRef(typeRef, UserTypeRef)) await this.handleUpdatedUser(cached, newEntity);
			await this.storage.put(newEntity);
			return update;
		} catch (e) {
			if (isExpectedErrorForSynchronization(e)) {
				console.log(`Instance not found when processing update for ${JSON.stringify(update)}, deleting from the cache.`);
				await this.storage.deleteIfExists(typeRef, instanceListId, instanceId);
				return null;
			} else throw e;
		}
		return update;
	}
	async handleUpdatedUser(cached, newEntity) {
		const oldUser = cached;
		if (oldUser._id !== this.storage.getUserId()) return;
		const newUser = newEntity;
		const removedShips = difference(oldUser.memberships, newUser.memberships, (l, r) => l._id === r._id);
		for (const ship of removedShips) {
			console.log("Lost membership on ", ship._id, ship.groupType);
			await this.storage.deleteAllOwnedBy(ship.group);
		}
	}
	/**
	*
	* @returns {Array<Id>} the ids that are in cache range and therefore should be cached
	*/
	async getElementIdsInCacheRange(typeRef, listId, ids) {
		const ret = [];
		for (let i = 0; i < ids.length; i++) if (await this.storage.isElementIdInCacheRange(typeRef, listId, ids[i])) ret.push(ids[i]);
		return ret;
	}
	/**
	* to avoid excessive entity updates and inconsistent offline storages, we don't send entity updates for each mail set migrated mail.
	* instead we detect the mail set migration for each folder and drop its whole list from offline.
	*/
	async checkForMailSetMigration(typeRef, update) {
		if (update.operation !== OperationType.UPDATE || !isSameTypeRef(typeRef, MailFolderTypeRef)) return;
		const oldFolder = await this.storage.get(MailFolderTypeRef, update.instanceListId, update.instanceId);
		if (oldFolder != null && oldFolder.isMailSet) return;
		const updatedFolder = await this.entityRestClient.load(MailFolderTypeRef, [update.instanceListId, update.instanceId]);
		if (!updatedFolder.isMailSet) return;
		await this.storage.deleteWholeList(MailTypeRef, updatedFolder.mails);
		await this.storage.put(updatedFolder);
	}
	/**
	* Check if the given request should use the cache
	* @param typeRef typeref of the type
	* @param opts entity rest client options, if any
	* @return true if the cache can be used, false if a direct network request should be performed
	*/
	shouldUseCache(typeRef, opts) {
		if (isIgnoredType(typeRef)) return false;
		return opts?.queryParams?.version == null;
	}
};
/**
* Returns whether the error is expected for the cases where our local state might not be up-to-date with the server yet. E.g. we might be processing an update
* for the instance that was already deleted. Normally this would be optimized away but it might still happen due to timing.
*/
function isExpectedErrorForSynchronization(e) {
	return e instanceof NotFoundError || e instanceof NotAuthorizedError;
}
function expandId(id) {
	if (typeof id === "string") return {
		listId: null,
		elementId: id
	};
else {
		const [listId, elementId] = id;
		return {
			listId,
			elementId
		};
	}
}
function collapseId(listId, elementId) {
	if (listId != null) return [listId, elementId];
else return elementId;
}
function getUpdateInstanceId(update) {
	let instanceListId;
	if (update.instanceListId === "") instanceListId = null;
else instanceListId = update.instanceListId;
	return {
		instanceListId,
		instanceId: update.instanceId
	};
}
/**
* Check if a range request begins inside an existing range
*/
function isStartIdWithinRange(range, startId, typeModel) {
	return !firstBiggerThanSecond(startId, range.upper, typeModel) && !firstBiggerThanSecond(range.lower, startId, typeModel);
}
/**
* Check if a range request is going away from an existing range
* Assumes that the range request doesn't start inside the range
*/
function isRangeRequestAwayFromExistingRange(range, reverse, start, typeModel) {
	return reverse ? firstBiggerThanSecond(range.lower, start, typeModel) : firstBiggerThanSecond(start, range.upper, typeModel);
}
/**
* some types are completely ignored by the cache and always served from a request.
* Note:
* isCachedRangeType(ref) ---> !isIgnoredType(ref) but
* isIgnoredType(ref) -/-> !isCachedRangeType(ref) because of opted-in CustomId types.
*/
function isIgnoredType(typeRef) {
	return typeRef.app === "monitor" || IGNORED_TYPES.some((ref) => isSameTypeRef(typeRef, ref));
}
/**
* Checks if for the given type, that contains a customId,  caching is enabled.
*/
function isCachableCustomIdType(typeRef) {
	return CACHEABLE_CUSTOMID_TYPES.some((ref) => isSameTypeRef(typeRef, ref));
}
/**
* Ranges for customId types are normally not cached, but some are opted in.
* Note:
* isCachedRangeType(ref) ---> !isIgnoredType(ref) but
* isIgnoredType(ref) -/-> !isCachedRangeType(ref)
*/
function isCachedRangeType(typeModel, typeRef) {
	return !isIgnoredType(typeRef) && isGeneratedIdType(typeModel) || isCachableCustomIdType(typeRef);
}
function isGeneratedIdType(typeModel) {
	return typeModel.values._id.type === ValueType.GeneratedId;
}

//#endregion
//#region ../src/common/api/worker/rest/EntityRestClient.ts
assertWorkerOrNode();
function typeRefToPath(typeRef) {
	return `/rest/${typeRef.app}/${typeRef.type.toLowerCase()}`;
}
let CacheMode = function(CacheMode$1) {
	/** Prefer cached value if it's there, or fall back to network and write it to cache. */
	CacheMode$1[CacheMode$1["ReadAndWrite"] = 0] = "ReadAndWrite";
	/**
	* Always retrieve from the network, but still save to cache.
	*
	* NOTE: This cannot be used with ranged requests.
	*/
	CacheMode$1[CacheMode$1["WriteOnly"] = 1] = "WriteOnly";
	/** Prefer cached value, but in case of a cache miss, retrieve the value from network without writing it to cache. */
	CacheMode$1[CacheMode$1["ReadOnly"] = 2] = "ReadOnly";
	return CacheMode$1;
}({});
function getCacheModeBehavior(cacheMode) {
	switch (cacheMode ?? CacheMode.ReadAndWrite) {
		case CacheMode.ReadAndWrite: return {
			readsFromCache: true,
			writesToCache: true
		};
		case CacheMode.WriteOnly: return {
			readsFromCache: false,
			writesToCache: true
		};
		case CacheMode.ReadOnly: return {
			readsFromCache: true,
			writesToCache: false
		};
	}
}
var EntityRestClient = class {
	get _crypto() {
		return this.lazyCrypto();
	}
	constructor(authDataProvider, restClient, lazyCrypto, instanceMapper, blobAccessTokenFacade) {
		this.authDataProvider = authDataProvider;
		this.restClient = restClient;
		this.lazyCrypto = lazyCrypto;
		this.instanceMapper = instanceMapper;
		this.blobAccessTokenFacade = blobAccessTokenFacade;
	}
	async load(typeRef, id, opts = {}) {
		const { listId, elementId } = expandId(id);
		const { path, queryParams, headers, typeModel } = await this._validateAndPrepareRestRequest(typeRef, listId, elementId, opts.queryParams, opts.extraHeaders, opts.ownerKeyProvider);
		const json = await this.restClient.request(path, HttpMethod.GET, {
			queryParams,
			headers,
			responseType: MediaType.Json,
			baseUrl: opts.baseUrl
		});
		const entity = JSON.parse(json);
		const migratedEntity = await this._crypto.applyMigrations(typeRef, entity);
		const sessionKey = await this.resolveSessionKey(opts.ownerKeyProvider, migratedEntity, typeModel);
		const instance = await this.instanceMapper.decryptAndMapToInstance(typeModel, migratedEntity, sessionKey);
		return this._crypto.applyMigrationsForInstance(instance);
	}
	async resolveSessionKey(ownerKeyProvider, migratedEntity, typeModel) {
		try {
			if (ownerKeyProvider && migratedEntity._ownerEncSessionKey) {
				const ownerKey = await ownerKeyProvider(Number(migratedEntity._ownerKeyVersion ?? 0));
				return this._crypto.resolveSessionKeyWithOwnerKey(migratedEntity, ownerKey);
			} else return await this._crypto.resolveSessionKey(typeModel, migratedEntity);
		} catch (e) {
			if (e instanceof SessionKeyNotFoundError) {
				console.log(`could not resolve session key for instance of type ${typeModel.app}/${typeModel.name}`, e);
				return null;
			} else throw e;
		}
	}
	async loadRange(typeRef, listId, start, count, reverse, opts = {}) {
		const rangeRequestParams = {
			start: String(start),
			count: String(count),
			reverse: String(reverse)
		};
		const { path, headers, typeModel, queryParams } = await this._validateAndPrepareRestRequest(typeRef, listId, null, Object.assign(rangeRequestParams, opts.queryParams), opts.extraHeaders, opts.ownerKeyProvider);
		if (typeModel.type !== Type.ListElement) throw new Error("only ListElement types are permitted");
		const json = await this.restClient.request(path, HttpMethod.GET, {
			queryParams,
			headers,
			responseType: MediaType.Json,
			baseUrl: opts.baseUrl,
			suspensionBehavior: opts.suspensionBehavior
		});
		return this._handleLoadMultipleResult(typeRef, JSON.parse(json));
	}
	async loadMultiple(typeRef, listId, elementIds, ownerEncSessionKeyProvider, opts = {}) {
		const { path, headers } = await this._validateAndPrepareRestRequest(typeRef, listId, null, opts.queryParams, opts.extraHeaders, opts.ownerKeyProvider);
		const idChunks = splitInChunks(LOAD_MULTIPLE_LIMIT, elementIds);
		const typeModel = await resolveTypeReference(typeRef);
		const loadedChunks = await pMap(idChunks, async (idChunk) => {
			let queryParams = { ids: idChunk.join(",") };
			let json;
			if (typeModel.type === Type.BlobElement) json = await this.loadMultipleBlobElements(listId, queryParams, headers, path, typeRef, opts.suspensionBehavior);
else json = await this.restClient.request(path, HttpMethod.GET, {
				queryParams,
				headers,
				responseType: MediaType.Json,
				baseUrl: opts.baseUrl,
				suspensionBehavior: opts.suspensionBehavior
			});
			return this._handleLoadMultipleResult(typeRef, JSON.parse(json), ownerEncSessionKeyProvider);
		});
		return loadedChunks.flat();
	}
	async loadMultipleBlobElements(archiveId, queryParams, headers, path, typeRef, suspensionBehavior) {
		if (archiveId == null) throw new Error("archiveId must be set to load BlobElementTypes");
		const doBlobRequest = async () => {
			const blobServerAccessInfo = await this.blobAccessTokenFacade.requestReadTokenArchive(archiveId);
			const additionalRequestParams = Object.assign({}, headers, queryParams);
			const allParams = await this.blobAccessTokenFacade.createQueryParams(blobServerAccessInfo, additionalRequestParams, typeRef);
			return tryServers(blobServerAccessInfo.servers, async (serverUrl) => this.restClient.request(path, HttpMethod.GET, {
				queryParams: allParams,
				headers: {},
				responseType: MediaType.Json,
				baseUrl: serverUrl,
				noCORS: true,
				suspensionBehavior
			}), `can't load instances from server `);
		};
		const doEvictToken = () => this.blobAccessTokenFacade.evictArchiveToken(archiveId);
		return doBlobRequestWithRetry(doBlobRequest, doEvictToken);
	}
	async _handleLoadMultipleResult(typeRef, loadedEntities, ownerEncSessionKeyProvider) {
		const model = await resolveTypeReference(typeRef);
		if (isSameTypeRef(typeRef, PushIdentifierTypeRef)) await pMap(loadedEntities, (instance) => this._crypto.applyMigrations(typeRef, instance), { concurrency: 5 });
		return pMap(loadedEntities, (instance) => {
			return this._decryptMapAndMigrate(instance, model, ownerEncSessionKeyProvider);
		}, { concurrency: 5 });
	}
	async _decryptMapAndMigrate(instance, model, ownerEncSessionKeyProvider) {
		let sessionKey;
		if (ownerEncSessionKeyProvider) sessionKey = await this._crypto.decryptSessionKey(instance, await ownerEncSessionKeyProvider(getElementId(instance)));
else try {
			sessionKey = await this._crypto.resolveSessionKey(model, instance);
		} catch (e) {
			if (e instanceof SessionKeyNotFoundError) {
				console.log("could not resolve session key", e, e.message, e.stack);
				sessionKey = null;
			} else throw e;
		}
		const decryptedInstance = await this.instanceMapper.decryptAndMapToInstance(model, instance, sessionKey);
		return this._crypto.applyMigrationsForInstance(decryptedInstance);
	}
	async setup(listId, instance, extraHeaders, options) {
		const typeRef = instance._type;
		const { typeModel, path, headers, queryParams } = await this._validateAndPrepareRestRequest(typeRef, listId, null, undefined, extraHeaders, options?.ownerKey);
		if (typeModel.type === Type.ListElement) {
			if (!listId) throw new Error("List id must be defined for LETs");
		} else if (listId) throw new Error("List id must not be defined for ETs");
		const sk = await this._crypto.setNewOwnerEncSessionKey(typeModel, instance, options?.ownerKey);
		const encryptedEntity = await this.instanceMapper.encryptAndMapToLiteral(typeModel, instance, sk);
		const persistencePostReturn = await this.restClient.request(path, HttpMethod.POST, {
			baseUrl: options?.baseUrl,
			queryParams,
			headers,
			body: JSON.stringify(encryptedEntity),
			responseType: MediaType.Json
		});
		return JSON.parse(persistencePostReturn).generatedId;
	}
	async setupMultiple(listId, instances) {
		const count = instances.length;
		if (count < 1) return [];
		const instanceChunks = splitInChunks(POST_MULTIPLE_LIMIT, instances);
		const typeRef = instances[0]._type;
		const { typeModel, path, headers } = await this._validateAndPrepareRestRequest(typeRef, listId, null, undefined, undefined, undefined);
		if (typeModel.type === Type.ListElement) {
			if (!listId) throw new Error("List id must be defined for LETs");
		} else if (listId) throw new Error("List id must not be defined for ETs");
		const errors = [];
		const failedInstances = [];
		const idChunks = await pMap(instanceChunks, async (instanceChunk) => {
			try {
				const encryptedEntities = await pMap(instanceChunk, async (e) => {
					const sk = await this._crypto.setNewOwnerEncSessionKey(typeModel, e);
					return this.instanceMapper.encryptAndMapToLiteral(typeModel, e, sk);
				});
				const queryParams = { count: String(instanceChunk.length) };
				const persistencePostReturn = await this.restClient.request(path, HttpMethod.POST, {
					queryParams,
					headers,
					body: JSON.stringify(encryptedEntities),
					responseType: MediaType.Json
				});
				return this.parseSetupMultiple(persistencePostReturn);
			} catch (e) {
				if (e instanceof PayloadTooLargeError) {
					const returnedIds = await pMap(instanceChunk, (instance) => {
						return this.setup(listId, instance).catch((e$1) => {
							errors.push(e$1);
							failedInstances.push(instance);
						});
					});
					return returnedIds.filter(Boolean);
				} else {
					errors.push(e);
					failedInstances.push(...instanceChunk);
					return [];
				}
			}
		});
		if (errors.length) {
			if (errors.some(isOfflineError)) throw new ConnectionError("Setup multiple entities failed");
			throw new SetupMultipleError("Setup multiple entities failed", errors, failedInstances);
		} else return idChunks.flat();
	}
	async update(instance, options) {
		if (!instance._id) throw new Error("Id must be defined");
		const { listId, elementId } = expandId(instance._id);
		const { path, queryParams, headers, typeModel } = await this._validateAndPrepareRestRequest(instance._type, listId, elementId, undefined, undefined, options?.ownerKeyProvider);
		const sessionKey = await this.resolveSessionKey(options?.ownerKeyProvider, instance, typeModel);
		const encryptedEntity = await this.instanceMapper.encryptAndMapToLiteral(typeModel, instance, sessionKey);
		await this.restClient.request(path, HttpMethod.PUT, {
			baseUrl: options?.baseUrl,
			queryParams,
			headers,
			body: JSON.stringify(encryptedEntity),
			responseType: MediaType.Json
		});
	}
	async erase(instance, options) {
		const { listId, elementId } = expandId(instance._id);
		const { path, queryParams, headers } = await this._validateAndPrepareRestRequest(instance._type, listId, elementId, undefined, options?.extraHeaders, undefined);
		await this.restClient.request(path, HttpMethod.DELETE, {
			queryParams,
			headers
		});
	}
	async _validateAndPrepareRestRequest(typeRef, listId, elementId, queryParams, extraHeaders, ownerKey) {
		const typeModel = await resolveTypeReference(typeRef);
		_verifyType(typeModel);
		if (ownerKey == undefined && !this.authDataProvider.isFullyLoggedIn() && typeModel.encrypted) throw new LoginIncompleteError(`Trying to do a network request with encrypted entity but is not fully logged in yet, type: ${typeModel.name}`);
		let path = typeRefToPath(typeRef);
		if (listId) path += "/" + listId;
		if (elementId) path += "/" + elementId;
		const headers = Object.assign({}, this.authDataProvider.createAuthHeaders(), extraHeaders);
		if (Object.keys(headers).length === 0) throw new NotAuthenticatedError("user must be authenticated for entity requests");
		headers.v = typeModel.version;
		return {
			path,
			queryParams,
			headers,
			typeModel
		};
	}
	/**
	* for the admin area (no cache available)
	*/
	entityEventsReceived(batch) {
		return Promise.resolve(batch.events);
	}
	getRestClient() {
		return this.restClient;
	}
	parseSetupMultiple(result) {
		try {
			return JSON.parse(result).map((r) => r.generatedId);
		} catch (e) {
			throw new Error(`Invalid response: ${result}, ${e}`);
		}
	}
};
async function tryServers(servers, mapper, errorMsg) {
	let index = 0;
	let error = null;
	for (const server of servers) {
		try {
			return await mapper(server.url, index);
		} catch (e) {
			if (e instanceof ConnectionError || e instanceof InternalServerError || e instanceof NotFoundError) {
				console.log(`${errorMsg} ${server.url}`, e);
				error = e;
			} else throw e;
		}
		index++;
	}
	throw error;
}
async function doBlobRequestWithRetry(doBlobRequest, doEvictTokenBeforeRetry) {
	return doBlobRequest().catch(
		// in case one of the chunks could not be uploaded because of an invalid/expired token we upload all chunks again in order to guarantee that they are uploaded to the same archive.
		// we don't have to take care of already uploaded chunks, as they are unreferenced and will be cleaned up by the server automatically.
		ofClass(NotAuthorizedError, (e) => {
			doEvictTokenBeforeRetry();
			return doBlobRequest();
		})
);
}
function getIds(instance, typeModel) {
	if (!instance._id) throw new Error("Id must be defined");
	let listId = null;
	let id;
	if (typeModel.type === Type.ListElement) {
		listId = instance._id[0];
		id = instance._id[1];
	} else id = instance._id;
	return {
		listId,
		id
	};
}

//#endregion
export { CacheMode, ConnectMode, CustomCacheHandlerMap, CustomCalendarEventCacheHandler, DefaultEntityRestCache, EXTEND_RANGE_MIN_CHUNK_SIZE, EntityRestClient, EventBusClient, OfflineStorage, WebWorkerTransport, WsConnectionState, bootstrapWorker, customIdToBase64Url, customTypeDecoders, customTypeEncoders, decode, doBlobRequestWithRetry, encode, ensureBase64Ext, expandId, exposeLocalDelayed, exposeRemote, getIds, tryServers, typeRefToPath };
//# sourceMappingURL=EntityRestClient--6dT7ZRF.js.map